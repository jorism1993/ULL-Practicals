{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from random import randint\n",
    "import matplotlib\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pickle\n",
    "import dill\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load InferSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:367: SourceChangeWarning: source code of class 'models.BLSTMEncoder' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:367: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 2196016\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# if you are on GPU (encoding ~1000 sentences/s, default)\n",
    "# if you are on CPU (~40 sentences/s)\n",
    "infersent = torch.load('infersent.allnli.pickle', map_location=lambda storage, loc: storage)\n",
    "glove_path = 'SentEval/pretrained/glove.840B.300d.txt'\n",
    "infersent.set_glove_path(glove_path)\n",
    "infersent.build_vocab_k_words(K=5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('europarl_unigram.pickle', 'rb') as f:\n",
    "    unigram = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sent = 'Mark rutte is the president of the netherlands'\n",
    "_, _,y = infersent.visualize(my_sent,tokenize=True,visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_path = 'SkipGram/SG/skipgram.300d.txt'\n",
    "\n",
    "def load_skipgram(path):\n",
    "    \n",
    "    word_vec = {}\n",
    "    \n",
    "    with open(path,'r') as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "     \n",
    "    # Convert to defaultdict to return 0 if a word is unknown\n",
    "    word_vec = defaultdict(lambda: word_vec['UNK'], word_vec)\n",
    "                \n",
    "    return word_vec\n",
    "\n",
    "skipgram_word_vec = load_skipgram(skipgram_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_batcher(params, batch, func):\n",
    "    '''Implements the main loop of getting a batch of embeddings from the EmbedAlign model.\n",
    "    \n",
    "         _func_: should be a function that takes a [batch_size, sentence_length, embeddings_dim] matrix\n",
    "         and a list with the words in the sentence as arguments: func(z_batch, sentence). This function\n",
    "         should return a [1, embeddings_dim] matrix with the sentence representation'''\n",
    "    batch = [sent if sent != [] else ['.'] for sent in batch]\n",
    "    embeddings = []\n",
    "    for sent in batch:\n",
    "        \n",
    "        sent = [word.lower() for word in sent if word in infersent.word_vec]\n",
    "        \n",
    "        if len(sent) == 0:\n",
    "            sent_vec = skipgram_word_vec['UNK']\n",
    "        else:\n",
    "            z_batch1 = np.zeros([len(sent),300])\n",
    "\n",
    "            for i,word in enumerate(sent):\n",
    "                z_batch1[i,:] = skipgram_word_vec[word]\n",
    "\n",
    "            z_batch1 = np.expand_dims(z_batch1,0)\n",
    "\n",
    "            # Sentence embedding is a function of the words in a sentence\n",
    "            # [1, z_dim]\n",
    "            sent_vec = func(z_batch1, sent)\n",
    "        \n",
    "        # check if there is any NaN in vector (they appear sometimes when there's padding)\n",
    "        if np.isnan(sent_vec.sum()):\n",
    "            sent_vec = np.nan_to_num(sent_vec)   \n",
    "            \n",
    "        embeddings.append(sent_vec)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def simple_mean_batcher(params, batch):\n",
    "    out = generic_batcher(params, batch, lambda x, _: np.mean(x, axis=1))\n",
    "    return out\n",
    "\n",
    "def sif_weighted_batcher(params, batch):  \n",
    "    \n",
    "    def sif(z_batch1, sent):\n",
    "        \n",
    "        # Hyperparameter value taken from https://openreview.net/pdf?id=SyK00v5xx\n",
    "        a = 10e-3\n",
    "        \n",
    "        # Gather the weights for this sentence (https://openreview.net/pdf?id=SyK00v5xx)\n",
    "        weights = [a / (a + unigram[word]) for word in sent]\n",
    "        \n",
    "        # every sequence starts with 2, which is the id for -NULL- (assuming that is padding)\n",
    "        # so it gets a weight of 0\n",
    "        #weights = [0] + weights\n",
    "        weights = np.array(weights)\n",
    "        \n",
    "        # Add dimension to weight array to match shape of z_batch1\n",
    "        weights = weights.reshape(-1, z_batch1.shape[1], 1)\n",
    "        \n",
    "        # Sentence embedding is the average of the words in a sentence (weighted by their unigram prob)\n",
    "        # [1, z_dim]\n",
    "        return np.mean(weights * z_batch1, 1)\n",
    "    \n",
    "\n",
    "    return generic_batcher(params, batch, sif)\n",
    "\n",
    "def infersent_batcher(params, batch):\n",
    "    \n",
    "    def infersent_weighting(z_batch1, sent):\n",
    "        # Calculate importance of words, and remove start and stop symbol\n",
    "        _,_,weights = infersent.visualize(' '.join(sent), tokenize=False, visualize=False)\n",
    "\n",
    "        #weights = redistribute_start_stop(weights)\n",
    "        \n",
    "        # every sequence starts with 2, which is the id for -NULL- (assuming that is padding)\n",
    "        # so it gets a weight of 0\n",
    "        weights = np.array(weights)\n",
    "        # Add dimensions to weight array to match shape of z_batch1\n",
    "        weights = weights.reshape(-1, z_batch1.shape[1], 1)\n",
    "        # Sentence embedding is the average of the words in a sentence\n",
    "        # [1, z_dim]\n",
    "        out = np.sum(weights * z_batch1, 1)\n",
    "        return out\n",
    "        \n",
    "    return generic_batcher(params, batch, infersent_weighting)\n",
    "\n",
    "\n",
    "def redistribute_start_stop(y):\n",
    "    '''Function to redistribute the start and stop words importance over the rest'''\n",
    "    \n",
    "    # Remove first and last element (start and stop symbol)\n",
    "    del y[0]\n",
    "    del y[-1]\n",
    "    \n",
    "    # convert to float\n",
    "    y = [float(i) for i in y]\n",
    "    \n",
    "    # Make it sum to 1.0 instead of 100.0\n",
    "    y = [i/100.0 for i in y]\n",
    "    \n",
    "    # Divide by the sum, to give all other words equall extra mass\n",
    "    y = [i/sum(y) for i in y]\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-24 16:44:35,498 : ***** Transfer task : MPQA *****\n",
      "\n",
      "\n",
      "2018-05-24 16:44:37,361 : Found 5454 words with word vectors, out of         6241 words\n",
      "2018-05-24 16:44:37,596 : Generating sentence embeddings\n",
      "2018-05-24 16:44:38,122 : Generated sentence embeddings\n",
      "2018-05-24 16:44:38,122 : Training sklearn-LogReg with (inner) 10-fold cross-validation\n",
      "2018-05-24 16:45:15,945 : Best param found at split 1: l2reg = 0.25                 with score 82.66\n",
      "2018-05-24 16:45:58,765 : Best param found at split 2: l2reg = 0.5                 with score 82.94\n",
      "2018-05-24 16:46:56,713 : Best param found at split 3: l2reg = 0.5                 with score 83.02\n",
      "2018-05-24 16:48:01,877 : Best param found at split 4: l2reg = 4                 with score 83.09\n",
      "2018-05-24 16:48:47,792 : Best param found at split 5: l2reg = 0.5                 with score 82.9\n",
      "2018-05-24 16:49:40,236 : Best param found at split 6: l2reg = 2                 with score 82.91\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "from __future__ import absolute_import, division, unicode_literals\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import logging\n",
    "import sklearn\n",
    "import SentEval.examples.data as data\n",
    "import os\n",
    "\n",
    "# Set PATHs\n",
    "# path to senteval\n",
    "PATH_TO_SENTEVAL = 'SentEval'\n",
    "# path to the NLP datasets \n",
    "PATH_TO_DATA = os.path.join('SentEval','data')\n",
    "# path to skipgram embeddings\n",
    "PATH_TO_VEC = os.path.join('SkipGram','SG','skipgram.300d.txt')\n",
    "\n",
    "\n",
    "# import SentEval\n",
    "sys.path.insert(0, PATH_TO_SENTEVAL)\n",
    "import senteval\n",
    "\n",
    "\n",
    "def prepare(params, samples):\n",
    "    \"\"\"\n",
    "    In this example we are going to load Glove, \n",
    "    here you will initialize your model.\n",
    "    remember to add what you model needs into the params dictionary\n",
    "    \"\"\"\n",
    "    _, params.word2id = data.create_dictionary(samples)\n",
    "    # load glove/word2vec format \n",
    "    params.word_vec = data.get_wordvec(PATH_TO_VEC, params.word2id)\n",
    "    #params.word_vec = defaultdict(lambda: params.word_vec['UNK'], params.word_vec)\n",
    "    # dimensionality of glove embeddings\n",
    "    params.wvec_dim = 300\n",
    "    return\n",
    "\n",
    "\n",
    "# Set params for SentEval\n",
    "# we use logistic regression (usepytorch: Fasle) and kfold 10\n",
    "# In this dictionary you can add extra information that you model needs for initialization\n",
    "# for example the path to a dictionary of indices, of hyper parameters\n",
    "# this dictionary is passed to the batched and the prepare fucntions\n",
    "params_senteval = {'task_path': PATH_TO_DATA, 'usepytorch': False, 'kfold': 10}\n",
    "# this is the config for the NN classifier but we are going to use scikit-learn logistic regression with 10 kfold\n",
    "# usepytorch = False \n",
    "#params_senteval['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "#                                 'tenacity': 3, 'epoch_size': 2}\n",
    "\n",
    "# Set up logger\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # senteval prints the results and returns a dictionary with the scores\n",
    "    all_results = []\n",
    "    for batcher in [simple_mean_batcher,sif_weighted_batcher,infersent_batcher]:\n",
    "        se = senteval.engine.SE(params_senteval, batcher, prepare)\n",
    "\n",
    "        # here you define the NLP taks that your embedding model is going to be evaluated\n",
    "        # in (https://arxiv.org/abs/1802.05883) we use the following :\n",
    "        # SICKRelatedness (Sick-R) needs torch cuda to work (even when using logistic regression), \n",
    "        # but STS14 (semantic textual similarity) is a similar type of semantic task\n",
    "    #     transfer_tasks = ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC',\n",
    "    #                       'MRPC', 'SICKEntailment']\n",
    "        transfer_tasks = ['MPQA','MR', 'CR',  'SUBJ', 'SST2', 'TREC',\n",
    "                      'MRPC', 'SICKEntailment']\n",
    "        # senteval prints the results and returns a dictionary with the scores\n",
    "        results = se.eval(transfer_tasks)\n",
    "        all_results.append(results)\n",
    "        pprint.pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
