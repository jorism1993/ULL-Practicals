{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First pratical of the Unsupervised Language Learning course 2017-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joris Mollinga: 11871431 \n",
    "\n",
    "Aron Hammond: 10437215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sklearn\n",
    "import pprint\n",
    "from collections import deque\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a,b):\n",
    "    t = np.dot(a,b)\n",
    "    n = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "    return t/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the word embeddings file\n",
    "def readWORDSfile(fileName):\n",
    "    word2vec = []\n",
    "    \n",
    "    w2i = {}\n",
    "    i2w = {}\n",
    "\n",
    "    with open (fileName,'r',encoding=\"utf8\") as emb:\n",
    "        i = 0\n",
    "        for line in emb:\n",
    "            row = line.split()\n",
    "            word = row[0]\n",
    "            embedding = row[1:]\n",
    "\n",
    "            # Convert to floats and assign in dictionary\n",
    "            word2vec.append([float(i) for i in embedding])\n",
    "            \n",
    "            i2w[i] = word\n",
    "            w2i[word] = i\n",
    "            i += 1\n",
    "            \n",
    "    return np.array(word2vec),i2w,w2i\n",
    "\n",
    "def read2000words(fileName):\n",
    "        \n",
    "    with open (fileName,'r') as nouns:\n",
    "        words = nouns.readlines()\n",
    "        \n",
    "    words = [i.strip() for i in words]\n",
    "            \n",
    "    return words\n",
    "\n",
    "def readMEN(fileName):\n",
    "    judgements = []\n",
    "    \n",
    "    with open (fileName,'r') as file:\n",
    "        for line in file:\n",
    "            judgements.append(line.split())\n",
    "    \n",
    "    return judgements\n",
    "\n",
    "def readSimLex(fileName):\n",
    "    judgements = []\n",
    "    \n",
    "    with open (fileName,'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i == 0: continue\n",
    "            judgement = line.split('\\t')\n",
    "            judgements.append([judgement[0], judgement[1], judgement[3]])\n",
    "            \n",
    "    return judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec, W2V_i2w, W2V_w2i = readWORDSfile('deps.WORDS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2, bow2_i2w, bow2_w2i = readWORDSfile('bow2.WORDS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow5, bow5_i2w, bow5_w2i = readWORDSfile('bow5.WORDS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = read2000words('2000_nouns_sorted.txt')\n",
    "w2v_nouns = []\n",
    "bow2_nouns = []\n",
    "bow5_nouns = []\n",
    "removeList = []\n",
    "\n",
    "for word in words:\n",
    "    try:\n",
    "        w2v_nouns.append(word2vec[W2V_w2i[word]])\n",
    "        bow2_nouns.append(bow2[bow2_w2i[word]])\n",
    "        bow5_nouns.append(bow5[bow5_w2i[word]])\n",
    "    except:\n",
    "        removeList.append(word)\n",
    "\n",
    "for word in removeList:\n",
    "    words.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_word(word,embeddings,i2w,n=1):\n",
    "    if n == 1:\n",
    "        sim = [cosine_sim(embeddings[i],word) for i in range(0,len(embeddings))]\n",
    "        idx = sim.index(max(sim))\n",
    "        return i2w[idx]\n",
    "    else:\n",
    "        sim = [cosine_sim(embeddings[i],word) for i in range(0,len(embeddings))]\n",
    "        idx = sorted(range(len(sim)), key=lambda i: sim[i])[-n:]\n",
    "        return list(reversed([i2w[i] for i in idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "men = readMEN(os.path.join('MEN', 'MEN_dataset_natural_form_full'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex = readSimLex(os.path.join('SimLex-999', 'SimLex-999.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MEN': {'Pearson': array([ 0.67769822,  0.70823625,  0.5974016 ]),\n",
      "         'Spearman': array([ 0.69990476,  0.72316866,  0.61782279])},\n",
      " 'SimLex': {'Pearson': array([ 0.42845866,  0.3756006 ,  0.46190134]),\n",
      "            'Spearman': array([ 0.41414577,  0.36739614,  0.44564093])}}\n"
     ]
    }
   ],
   "source": [
    "def similarity(word1, word2, model, w2i):\n",
    "    '''Compute the cosine similarity of the embeddings of word1 and word2 under the supplied model'''\n",
    "    \n",
    "    if not word1 in w2i or not word2 in w2i:\n",
    "        return 0\n",
    "    \n",
    "    emb1 = model[w2i[word1]]\n",
    "    emb2 = model[w2i[word2]]\n",
    "    \n",
    "    unit_emb1 = np.array(emb1) / np.linalg.norm(emb1)\n",
    "    unit_emb2 = np.array(emb2) / np.linalg.norm(emb2)\n",
    "    \n",
    "    return np.dot(unit_emb1, unit_emb2)\n",
    "\n",
    "def evaluate_models_on_set(judgements):\n",
    "    result = []\n",
    "    used = []\n",
    "    \n",
    "    for pair in judgements:\n",
    "        word1, word2, score = pair\n",
    "        bow2sim = similarity(word1, word2, bow2, bow2_w2i)\n",
    "        bow5sim = similarity(word1, word2, bow5, bow5_w2i)\n",
    "        depssim = similarity(word1, word2, word2vec, W2V_w2i)\n",
    "        \n",
    "        row = [bow2sim, bow5sim, depssim]\n",
    "                \n",
    "        if not all(row):\n",
    "            continue\n",
    "        \n",
    "        result.append(row)\n",
    "        used.append(pair)\n",
    "    \n",
    "    return np.array(result, dtype=np.float64), used\n",
    "\n",
    "def pearson(results, scores):\n",
    "    corr = []\n",
    "    for i in range(np.size(results, 1)):\n",
    "        corr.append(pearsonr(results[:,i], scores)[0])\n",
    "        \n",
    "    return np.array(corr)    \n",
    "\n",
    "# Compute cosine similarity between the words using the three models\n",
    "# words that don't occur in the models are skipped and removed from\n",
    "# the evaluation set.\n",
    "MENResults, men = evaluate_models_on_set(men)\n",
    "SimLexResults, simlex = evaluate_models_on_set(simlex)\n",
    "    \n",
    "# Evaluate the model-produced similarities against human judgements in terms of \n",
    "# Pearson and Spearman correlation coefficients.\n",
    "\n",
    "# Collect human judgements (last column of dataset)\n",
    "MENScores = np.array([judgement[-1] for judgement in men], dtype=np.float64)\n",
    "SimLexScores = np.array([judgement[-1] for judgement in simlex], dtype=np.float64)\n",
    "\n",
    "correlations = {\"MEN\" : {\n",
    "                   \"Spearman\" : [],\n",
    "                   \"Pearson\" : []},\n",
    "               \"SimLex\" : {\n",
    "                   \"Spearman\" : [],\n",
    "                   \"Pearson\" : []\n",
    "               }}\n",
    "\n",
    "correlations[\"MEN\"][\"Spearman\"] = spearmanr(MENResults, MENScores, axis = 0).correlation[0:3,-1]\n",
    "correlations[\"MEN\"][\"Pearson\"] = pearson(MENResults, MENScores)\n",
    "correlations[\"SimLex\"][\"Spearman\"] = spearmanr(SimLexResults, SimLexScores, axis = 0).correlation[0:3,-1]\n",
    "correlations[\"SimLex\"][\"Pearson\"] = pearson(SimLexResults, SimLexScores)\n",
    "\n",
    "pprint.pprint(correlations)\n",
    "\n",
    "# Compare the performance of the three models on this task.\n",
    "\n",
    "#  Analyze the data qualitatively and report what are the differences in the kind\n",
    "# of similarity captured by the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for word in simlex:\n",
    "    if word in men:\n",
    "        count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_question_words(filename):\n",
    "    analogies = []\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            elements = line.split()\n",
    "            \n",
    "            # words in embedding models are all lowercased\n",
    "            elements = [i.lower() for i in elements]\n",
    "            \n",
    "            # analogies are divided into sections by a line of the form \": capital-common-contries\"\n",
    "            if not elements[0] == ':':\n",
    "                analogies.append(elements)\n",
    "    \n",
    "    return analogies\n",
    "\n",
    "def reciprocal_rank(target, candidates):    \n",
    "    try:\n",
    "        rank = candidates.index(target) + 1\n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "    return 1/rank\n",
    "\n",
    "question_words = read_question_words('questions-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "{'bow2': 0.7586666666666667,\n",
      " 'bow5': 0.7423333333333332,\n",
      " 'deps': 0.33349999999999996}\n",
      "{'bow2': 0.7, 'bow5': 0.68, 'deps': 0.29}\n"
     ]
    }
   ],
   "source": [
    "MRR = {\"bow2\" : 0, \"bow5\" : 0, \"deps\" : 0}\n",
    "acc = {\"bow2\" : 0, \"bow5\" : 0, \"deps\" : 0}\n",
    "\n",
    "model_data = {\"bow2\" : (bow2, bow2_i2w, bow2_w2i), \n",
    "              \"bow5\" : (bow5, bow5_i2w, bow5_w2i), \n",
    "              \"deps\" : (word2vec, W2V_i2w, W2V_w2i)}\n",
    "\n",
    "for label, model in model_data.items():\n",
    "    embeddings = model[0]\n",
    "    i2w = model[1]\n",
    "    w2i = model[2]\n",
    "        \n",
    "    RR = 0\n",
    "    correct = 0\n",
    "    sample_size = 100\n",
    "    sample = random.sample(question_words, sample_size)\n",
    "    \n",
    "    for i, analogy in enumerate(sample):\n",
    "        \n",
    "        # Find embeddings corresponding to the analogy\n",
    "        # but leave b_prime as the target\n",
    "        try:\n",
    "            a,a_prime,b,b_prime = analogy\n",
    "            a_idx, a_prime_idx, b_idx = w2i[a], w2i[a_prime], w2i[b]\n",
    "            a = embeddings[a_idx]\n",
    "            a_prime = embeddings[a_prime_idx]\n",
    "            b = embeddings[b_idx]\n",
    "        except:\n",
    "            sample_size = sample_size - 1\n",
    "            continue\n",
    "        \n",
    "        # Estimate for b_prime based on our model\n",
    "        b_prime_est = np.array(b) + (np.array(a_prime) - np.array(a))\n",
    "        b_prime_est = b_prime_est.reshape(1,-1)\n",
    "        \n",
    "        scores = cosine_similarity(embeddings, b_prime_est)\n",
    "        scores[b_idx] = 0\n",
    "        scores[a_idx] = 0\n",
    "        scores[a_prime_idx] = 0\n",
    "        sorted_idx = np.argsort(scores, axis=0)\n",
    "        \n",
    "        top5 = [i2w[idx] for idx in sorted_idx[-5:, 0]]\n",
    "        closest_words = list(reversed(top5))\n",
    "        \n",
    "        print(analogy)\n",
    "        print(closest_words)\n",
    "        print()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        \n",
    "        # Count correctly estimated words\n",
    "        if closest_words[0] == b_prime:\n",
    "            correct += 1\n",
    "        \n",
    "        # Sum up reciprocal rankings (0 if not in set)\n",
    "        RR += reciprocal_rank(b_prime, closest_words)\n",
    "    \n",
    "    # Average statistics over the entire set\n",
    "    MRR[label] = RR / sample_size\n",
    "    acc[label] = correct / sample_size\n",
    "        \n",
    "pprint.pprint(MRR)\n",
    "pprint.pprint(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visually inspect analogies, sorted by analogy rather than by model\n",
    "\n",
    "examples = read_question_words('example-analogies.txt')\n",
    "\n",
    "for analogy in examples:\n",
    "    for label, model in model_data.items():\n",
    "        embeddings = model[0]\n",
    "        i2w = model[1]\n",
    "        w2i = model[2]\n",
    "\n",
    "        # Find embeddings corresponding to the analogy\n",
    "        # but leave b_prime as the target\n",
    "        try:\n",
    "            a,a_prime,b,b_prime = analogy\n",
    "            a_idx, a_prime_idx, b_idx = w2i[a], w2i[a_prime], w2i[b]\n",
    "            a = embeddings[a_idx]\n",
    "            a_prime = embeddings[a_prime_idx]\n",
    "            b = embeddings[b_idx]\n",
    "        except:\n",
    "            sample_size = sample_size - 1\n",
    "            continue\n",
    "\n",
    "        # Estimate for b_prime based on our model\n",
    "        b_prime_est = np.array(b) + (np.array(a_prime) - np.array(a))\n",
    "        b_prime_est = b_prime_est.reshape(1,-1)\n",
    "\n",
    "        scores = cosine_similarity(embeddings, b_prime_est)\n",
    "        scores[b_idx] = 0\n",
    "        scores[a_idx] = 0\n",
    "        scores[a_prime_idx] = 0\n",
    "        sorted_idx = np.argsort(scores, axis=0)\n",
    "\n",
    "        top5 = [i2w[idx] for idx in sorted_idx[-5:, 0]]\n",
    "        closest_words = list(reversed(top5))\n",
    "\n",
    "        print(label)\n",
    "        print(analogy)\n",
    "        print(closest_words)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA(n_components=2)\n",
    "Xw2v = pca2.fit_transform(word2vec)\n",
    "Xbow2 = pca2.fit_transform(bow2)\n",
    "Xbow5 = pca2.fit_transform(bow5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 10\n",
    "kmeans_w2v = KMeans(n_clusters = num_clusters,random_state = 0).fit(Xw2v)\n",
    "predict_w2v = kmeans_w2v.predict(Xw2v)\n",
    "centers_w2v = kmeans_w2v.cluster_centers_\n",
    "\n",
    "kmeans_bow2 = KMeans(n_clusters = num_clusters,random_state = 0).fit(Xbow2)\n",
    "predict_bow2 = kmeans_bow2.predict(Xbow2)\n",
    "centers_bow2 = kmeans_bow2.cluster_centers_\n",
    "\n",
    "kmeans_bow5 = KMeans(n_clusters = num_clusters,random_state = 0).fit(Xbow5)\n",
    "predict_bow5 = kmeans_bow5.predict(Xbow5)\n",
    "centers_bow5 = kmeans_bow5.cluster_centers_\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3,figsize=(15,5))\n",
    "ax1.scatter(Xw2v[:,0],Xw2v[:,1], c=predict_w2v,cmap= 'jet')\n",
    "ax1.scatter(centers_w2v[:,0],centers_w2v[:,1],c='1.0')\n",
    "ax1.set_title('Dependency based model')\n",
    "\n",
    "ax2.scatter(Xbow2[:,0],Xbow2[:,1], c=predict_bow2,cmap= 'jet')\n",
    "ax2.scatter(centers_bow2[:,0],centers_bow2[:,1],c='1.0')\n",
    "ax2.set_title('BOW-2 model')\n",
    "\n",
    "ax3.scatter(Xbow5[:,0],Xbow5[:,1], c=predict_bow5,cmap= 'jet')\n",
    "ax3.scatter(centers_bow5[:,0],centers_bow5[:,1],c='1.0')\n",
    "ax3.set_title('BOW-5 model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 200\n",
    "kmeans_w2v_nouns = KMeans(n_clusters = num_clusters,random_state = 0).fit(w2v_nouns)\n",
    "centers_w2v_nouns = kmeans_w2v_nouns.cluster_centers_\n",
    "predict_w2v_nouns = kmeans_w2v_nouns.predict(w2v_nouns)\n",
    "\n",
    "word_list = [[] for x in range(num_clusters)]\n",
    "assert len(predict_w2v_nouns) == len(words)\n",
    "\n",
    "for cluster,word in zip(predict_w2v_nouns,words):\n",
    "    word_list[cluster].append(word)\n",
    "    \n",
    "''' \n",
    "print_all_clusters = False\n",
    "if print_all_clusters:\n",
    "    for cluster in word_list:\n",
    "        print (cluster)\n",
    "        print ('')\n",
    "else:\n",
    "    samples = random.sample(word_list,10)\n",
    "    for sample in samples:\n",
    "        print (sample)\n",
    "        print ('')\n",
    "'''        \n",
    "for cluster in word_list:\n",
    "    if 'money' in cluster:\n",
    "        print (cluster)\n",
    "    elif 'italy' in cluster:\n",
    "        print (cluster)\n",
    "    elif 'scientist' in cluster:\n",
    "        print (cluster)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sne = 5000 # number of samples\n",
    "pca30 = PCA(n_components=50)\n",
    "\n",
    "random_sample = random.sample(list(word2vec.values()),n_sne)\n",
    "\n",
    "w2v_pca_30 = pca30.fit_transform(random_sample)\n",
    "print ('Cumulative explained variation for 50 principal components: {}'.format(np.sum(pca30.explained_variance_ratio_)))\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "w2v_tsne_pca_results = tsne.fit_transform(w2v_pca_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_tsne = KMeans(n_clusters = 40,random_state = 0).fit(w2v_tsne_pca_results)\n",
    "predict_tsne = kmeans_tsne.predict(w2v_tsne_pca_results)\n",
    "centers_tsne = kmeans_tsne.cluster_centers_\n",
    "\n",
    "plt.scatter(w2v_tsne_pca_results[:,0],w2v_tsne_pca_results[:,1], c=predict_tsne, cmap= 'jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accountant', 'analyst', 'commentator', 'consultant', 'critic', 'engineer', 'expert', 'historian', 'lawyer', 'practitioner', 'reporter', 'researcher', 'resident', 'scholar', 'scientist', 'specialist', 'student', 'teacher']\n",
      "['card', 'cash', 'cheque', 'deposit', 'desk', 'fortune', 'gift', 'mail', 'money', 'packet', 'payment', 'receipt', 'ticket']\n",
      "['britain', 'england', 'europe', 'france', 'germany', 'iraq', 'ireland', 'israel', 'italy', 'japan', 'kingdom', 'kong', 'rome', 'russia', 'scotland', 'spain']\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 200\n",
    "kmeans_bow2_nouns = KMeans(n_clusters = num_clusters,random_state = 0).fit(bow2_nouns)\n",
    "centers_bow2_nouns = kmeans_bow2_nouns.cluster_centers_\n",
    "predict_bow2_nouns = kmeans_bow2_nouns.predict(bow2_nouns)\n",
    "\n",
    "word_list = [[] for x in range(num_clusters)]\n",
    "assert len(predict_bow2_nouns) == len(words)\n",
    "\n",
    "for cluster,word in zip(predict_bow2_nouns,words):\n",
    "    word_list[cluster].append(word)\n",
    "    \n",
    "''' \n",
    "print_all_clusters = False\n",
    "if print_all_clusters:\n",
    "    for cluster in word_list:\n",
    "        print (cluster)\n",
    "        print ('')\n",
    "else:\n",
    "    samples = random.sample(word_list,10)\n",
    "    for sample in samples:\n",
    "        print (sample)\n",
    "        print ('')\n",
    "'''        \n",
    "for cluster in word_list:\n",
    "    if 'money' in cluster:\n",
    "        print (cluster)\n",
    "    elif 'italy' in cluster:\n",
    "        print (cluster)\n",
    "    elif 'scientist' in cluster:\n",
    "        print (cluster)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 200\n",
    "kmeans_bow5_nouns = KMeans(n_clusters = num_clusters,random_state = 0).fit(bow5_nouns)\n",
    "centers_bow5_nouns = kmeans_bow5_nouns.cluster_centers_\n",
    "predict_bow5_nouns = kmeans_bow5_nouns.predict(bow5_nouns)\n",
    "\n",
    "word_list = [[] for x in range(num_clusters)]\n",
    "assert len(predict_bow5_nouns) == len(words)\n",
    "\n",
    "for cluster,word in zip(predict_bow5_nouns,words):\n",
    "    word_list[cluster].append(word)\n",
    "\n",
    "''' \n",
    "print_all_clusters = False\n",
    "if print_all_clusters:\n",
    "    for cluster in word_list:\n",
    "        print (cluster)\n",
    "        print ('')\n",
    "else:\n",
    "    samples = random.sample(word_list,10)\n",
    "    for sample in samples:\n",
    "        print (sample)\n",
    "        print ('')\n",
    "'''        \n",
    "for cluster in word_list:\n",
    "    if 'money' in cluster:\n",
    "        print (cluster)\n",
    "    elif 'italy' in cluster:\n",
    "        print (cluster)\n",
    "    elif 'scientist' in cluster:\n",
    "        print (cluster)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
