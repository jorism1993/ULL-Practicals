{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First pratical of the Unsupervised Language Learning course 2017-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joris Mollinga: 11871431 \n",
    "\n",
    "Aron Hammond: 10437215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sklearn\n",
    "import pprint\n",
    "from collections import deque\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonrfrom sklearn.decomposition import PCA\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a,b):\n",
    "    t = np.dot(a,b)\n",
    "    n = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "    return t/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the word embeddings file\n",
    "def readWORDSfile(fileName):\n",
    "    word2vec = {}\n",
    "    \n",
    "    w2i = {}\n",
    "    i2w = {}\n",
    "\n",
    "    with open (fileName,'r',encoding=\"utf8\") as emb:\n",
    "        i = 0\n",
    "        for line in emb:\n",
    "            row = line.split()\n",
    "            word = row[0]\n",
    "            embedding = row[1:]\n",
    "\n",
    "            # Convert to floats and assign in dictionary\n",
    "            word2vec[word] = [float(i) for i in embedding] \n",
    "            \n",
    "            i2w[i] = word\n",
    "            w2i[word] = i\n",
    "            i += 1\n",
    "    return word2vec,i2w,w2i\n",
    "\n",
    "def read2000words(fileName):\n",
    "        \n",
    "    with open (fileName,'r') as nouns:\n",
    "        words = nouns.readlines()\n",
    "        \n",
    "    words = [i.split()[0] for i in words]\n",
    "            \n",
    "    return words\n",
    "\n",
    "def readMEN(fileName):\n",
    "    judgements = []\n",
    "    \n",
    "    with open (fileName,'r') as file:\n",
    "        for line in file:\n",
    "            judgements.append(line.split())\n",
    "    \n",
    "    return judgements\n",
    "\n",
    "def readSimLex(fileName):\n",
    "    judgements = []\n",
    "    \n",
    "    with open (fileName,'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i == 0: continue\n",
    "            judgement = line.split('\\t')\n",
    "            judgements.append([judgement[0], judgement[1], judgement[3]])\n",
    "            \n",
    "    return judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec, W2V_i2w, W2V_w2i = readWORDSfile('deps.WORDS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow2, bow2_i2w, bow2_w2i = readWORDSfile('bow2.WORDS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow5, bow5_i2w, bow5_w2i = readWORDSfile('bow5.WORDS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "Xw2v = pca.fit_transform(list(word2vec.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = read2000words('2000_nouns_sorted.txt')\n",
    "w2v_nouns = []\n",
    "bow2_nouns = []\n",
    "bow5_nouns = []\n",
    "removeList = []\n",
    "\n",
    "for word in words:\n",
    "    try:\n",
    "        w2v_nouns.append(word2vec[word])\n",
    "        bow2_nouns.append(bow2[word])\n",
    "        bow5_nouns.append(bow5[word])\n",
    "    except:\n",
    "        removeList.append(word)\n",
    "\n",
    "for word in removeList:\n",
    "    words.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_word(a,p,i2w,n=1):\n",
    "    if n == 1:\n",
    "        sim = [cosine_sim(p[i],a) for i in range(0,len(p))]\n",
    "        idx = sim.index(max(sim))\n",
    "        return i2w[idx]\n",
    "    else:\n",
    "        sim = [cosine_sim(p[i],a) for i in range(0,len(p))]\n",
    "        idx = sorted(range(len(sim)), key=lambda i: sim[i])[-n:]\n",
    "        return list(reversed([i2w[i] for i in idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 15\n",
    "kmeans_w2v = KMeans(n_clusters = num_clusters,random_state = 0).fit(Xw2v)\n",
    "predict_w2v = kmeans_w2v.predict(Xw2v)\n",
    "centers_w2v = kmeans_w2v.cluster_centers_\n",
    "\n",
    "plt.scatter(Xw2v[:,0],Xw2v[:,1], c=predict_w2v,cmap= 'jet')\n",
    "plt.scatter(centers_w2v[:,0],centers_w2v[:,1],c='b')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans_w2v_nouns = KMeans(n_clusters = 35,random_state = 0).fit(w2v_nouns)\n",
    "centers_w2v_nouns = kmeans_w2v_nouns.cluster_centers_\n",
    "\n",
    "for center in centers_w2v_nouns:\n",
    "    print (find_closest_word(center, w2v_nouns, words , n = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_bow2_nouns = KMeans(n_clusters = 50,random_state = 0).fit(bow2_nouns)\n",
    "centers_bow2_nouns = kmeans_bow2_nouns.cluster_centers_\n",
    "\n",
    "for center in centers_bow2_nouns:\n",
    "    print (find_closest_word(center, bow2_nouns, words , n = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_bow5_nouns = KMeans(n_clusters = 10,random_state = 0).fit(bow5_nouns)\n",
    "centers_bow5_nouns = kmeans_bow5_nouns.cluster_centers_\n",
    "\n",
    "for center in centers_bow5_nouns:\n",
    "    print (find_closest_word(center, bow5_nouns, words , n = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men = readMEN(os.path.join('MEN', 'MEN_dataset_natural_form_full'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex = readSimLex(os.path.join('SimLex-999', 'SimLex-999.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2, model):\n",
    "    '''Compute the cosine similarity of the embeddings of word1 and word2 under the supplied model'''\n",
    "    \n",
    "    if not word1 in model or not word2 in model:\n",
    "        return 0\n",
    "    \n",
    "    emb1 = model[word1]\n",
    "    emb2 = model[word2]\n",
    "    \n",
    "    unit_emb1 = emb1 / np.linalg.norm(emb1)\n",
    "    unit_emb2 = emb2 / np.linalg.norm(emb2)\n",
    "    \n",
    "    return np.dot(unit_emb1, unit_emb2)\n",
    "\n",
    "def evaluate_models_on_set(judgements):\n",
    "    result = []\n",
    "    used = []\n",
    "    \n",
    "    for pair in judgements:\n",
    "        word1, word2, score = pair\n",
    "        bow2sim = similarity(word1, word2, bow2)\n",
    "        bow5sim = similarity(word1, word2, bow5)\n",
    "        depssim = similarity(word1, word2, word2vec)\n",
    "        \n",
    "        row = [bow2sim, bow5sim, depssim]\n",
    "        \n",
    "        if not all(row):\n",
    "            continue\n",
    "        \n",
    "        result.append(row)\n",
    "        used.append(pair)\n",
    "    \n",
    "    return np.array(result, dtype=np.float64), used\n",
    "\n",
    "def pearson(results, scores):\n",
    "    corr = []\n",
    "    for i in range(np.size(results, 1)):\n",
    "        corr.append(pearsonr(results[:,i], scores)[0])\n",
    "        \n",
    "    return np.array(corr)\n",
    "\n",
    "def top5_similar(model, word):\n",
    "    top5 = deque()\n",
    "    \n",
    "    if word in model:\n",
    "        emb = model[word]\n",
    "    \n",
    "    \n",
    "\n",
    "# Compute cosine similarity between the words using the three models\n",
    "# words that don't occur in the models are skipped and removed from\n",
    "# the evaluation set.\n",
    "MENResults, men = evaluate_models_on_set(men)\n",
    "SimLexResults, simlex = evaluate_models_on_set(simlex)\n",
    "    \n",
    "# Evaluate the model-produced similarities against human judgements in terms of \n",
    "# Pearson and Spearman correlation coefficients.\n",
    "\n",
    "# Collect human judgements (last column of dataset)\n",
    "MENScores = np.array([judgement[-1] for judgement in men], dtype=np.float64)\n",
    "SimLexScores = np.array([judgement[-1] for judgement in simlex], dtype=np.float64)\n",
    "\n",
    "correlations = {\"MEN\" : {\n",
    "                   \"Spearman\" : [],\n",
    "                   \"Pearson\" : []},\n",
    "               \"SimLex\" : {\n",
    "                   \"Spearman\" : [],\n",
    "                   \"Pearson\" : []\n",
    "               }}\n",
    "\n",
    "correlations[\"MEN\"][\"Spearman\"] = spearmanr(MENResults, MENScores, axis = 0).correlation[0:3,-1]\n",
    "correlations[\"MEN\"][\"Pearson\"] = pearson(MENResults, MENScores)\n",
    "correlations[\"SimLex\"][\"Spearman\"] = spearmanr(SimLexResults, SimLexScores, axis = 0).correlation[0:3,-1]\n",
    "correlations[\"SimLex\"][\"Pearson\"] = pearson(SimLexResults, SimLexScores)\n",
    "\n",
    "pprint.pprint(correlations)\n",
    "\n",
    "# Compare the performance of the three models on this task.\n",
    "\n",
    "#  Analyze the data qualitatively and report what are the differences in the kind\n",
    "# of similarity captured by the three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at both Pearson and Spearman correlations, the similarity scores for \\texttt{bow5} seem to correlate the best with the MEN evaluations but worst with SimLex. This is most likeley due to the fact that SimLex focusses on similarity rather than relatedness and a larger window captures relatively more topical relations. As expected, the dependency based embeddings have the highest correlation on SimLex.\n",
    "What is curious, is that correlations with MEN are striclty larger than those on SimLex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
