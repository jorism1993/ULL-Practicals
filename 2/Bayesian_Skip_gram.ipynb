{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian Skip-gram",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1ayvXjgoO7W6IpsF2hAVq3aVlKQecbge_",
          "timestamp": 1526474474615
        },
        {
          "file_id": "1cIymPrqXYwCI_AFpHQm2iYeYF9VsHElv",
          "timestamp": 1525872474801
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Oim7Ho4SJBWL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Learning Word Representations"
      ]
    },
    {
      "metadata": {
        "id": "c1B5E-8_JBWN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2a24f9a6-7f9a-4b99-bcdd-1f129cd094fc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526563788681,
          "user_tz": -120,
          "elapsed": 17008,
          "user": {
            "displayName": "Aron Hammond",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106989373078986092985"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy as sc\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import io\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import itertools\n",
        "import os\n",
        "import subprocess\n",
        "import itertools\n",
        "import sys\n",
        "import time\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Volumes/Macintosh_HD/Users/Aron/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "1NV_0SaiJwJu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c0e84852-d882-437b-a56a-28a824fc3c19",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526563815027,
          "user_tz": -120,
          "elapsed": 3947,
          "user": {
            "displayName": "Aron Hammond",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106989373078986092985"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "from torch.distributions.kl import kl_divergence\n",
        "import torch.optim as optim\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.0\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_gbaioJzJBWV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def read_sentences(file):\n",
        "    '''Read text in file and tokenize sentences'''\n",
        "    sentences = []\n",
        "\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    \n",
        "    with open(file) as f:\n",
        "        for line in f.readlines():\n",
        "            tokens = line.split()\n",
        "            tokens = [token.lower() for token in tokens if token not in stopWords]\n",
        "            tokens = list(filter(lambda x: x not in string.punctuation, tokens))\n",
        "            sentences.append(tokens)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def make_window_vectors(sentence, idx, half_window, w2i):\n",
        "    # Boundaries of winodow\n",
        "    start_idx = max(idx - half_window, 0)\n",
        "    end_idx = min(len(sentence), idx + half_window + 1)\n",
        "\n",
        "    # Target\n",
        "    target_word = sentence[idx]\n",
        "\n",
        "    # Context\n",
        "    context = sentence[start_idx:idx] + sentence[idx+1:end_idx]\n",
        "\n",
        "    # Wrap data in Variable\n",
        "    target_tensor = Variable(torch.LongTensor([w2i[target_word]]))\n",
        "    context_tensor = Variable(torch.LongTensor([w2i[word] for word in context]))\n",
        "    \n",
        "    return target_tensor, context_tensor\n",
        "    \n",
        "\n",
        "def generate_bsg_data(tokenized_sentences, window_size, w2i):\n",
        "    \n",
        "    assert window_size % 2 == 0, \"Window size should be an even number\"\n",
        "    half_window = int(window_size / 2)\n",
        "    \n",
        "    for sentence in tokenized_sentences:  \n",
        "                \n",
        "        # Skip single token sentences as they have no context and break the\n",
        "        # model\n",
        "        if len(sentence) == 1: continue\n",
        "            \n",
        "        sentence = half_window * [\"<s>\"] + sentence + half_window * [\"</s>\"]\n",
        "        \n",
        "        for i in range(half_window, len(sentence) - half_window):\n",
        "\n",
        "            target_tensor, context_tensor = make_window_vectors(sentence, i, half_window, w2i)\n",
        "\n",
        "            yield target_tensor, context_tensor\n",
        "            \n",
        "def get_bsg_batch(generator, batch_size):\n",
        "    \n",
        "    targets = []\n",
        "    contexts = []\n",
        "    \n",
        "    for target, context in itertools.islice(generator, batch_size):\n",
        "        \n",
        "        targets.append(target)\n",
        "        contexts.append(context)\n",
        "        \n",
        "    targets = torch.stack(targets, 0)\n",
        "    context = torch.stack(contexts, 0)\n",
        "            \n",
        "    return targets, context\n",
        "            \n",
        "\n",
        "def create_vocabulary(corpus, n=10000):\n",
        "    \n",
        "    all_words = list(itertools.chain(*corpus))\n",
        "    top_n = set([i[0] for i in Counter(all_words).most_common(n)])\n",
        "    vocabulary = set(all_words).intersection(top_n)\n",
        " \n",
        "\n",
        "    # Placeholder for unknown words, start en end of senctence\n",
        "    vocabulary.add(\"<unk>\")\n",
        "    vocabulary.add(\"<s>\")\n",
        "    vocabulary.add(\"</s>\")\n",
        "\n",
        "    word2idx = {}\n",
        "    idx2word = {}\n",
        "\n",
        "    for (idx, word) in enumerate(list(vocabulary)):\n",
        "        word2idx[word] = idx\n",
        "        idx2word[idx] = word\n",
        "\n",
        "    # Return the ID for <unk> for new words\n",
        "    word2idx = defaultdict(lambda: word2idx[\"<unk>\"], word2idx)\n",
        "\n",
        "    vocabulary_size = len(vocabulary)\n",
        "\n",
        "    return word2idx, idx2word, vocabulary_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "umcJStM7JBWY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "dev_en = read_sentences(os.path.join('wa', 'dev.en'))\n",
        "training_en = read_sentences(os.path.join('hansards', 'training.en'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1lN2U6gaGRw9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Bayesian Skip Gram"
      ]
    },
    {
      "metadata": {
        "id": "pz0ps2Z8GQqG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class BayesianSkipGram(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embeddings_dim):\n",
        "        super(BayesianSkipGram, self).__init__()\n",
        "                \n",
        "        # Map from vocab_size to embedding space\n",
        "        # These are internal embeddings which we will\n",
        "        # not inspect for analysis\n",
        "        self.embed = torch.nn.Embedding(vocab_size, embeddings_dim, max_norm=1.0)\n",
        "        \n",
        "        # Linear mapping to mean\n",
        "        self.linear_mu = torch.nn.Linear(2*embeddings_dim, embeddings_dim)\n",
        "        \n",
        "        # Linear mapping to sigma\n",
        "        self.linear_sigma = torch.nn.Linear(2*embeddings_dim, embeddings_dim)\n",
        "        \n",
        "        # Linear mapping from latent space to vocabulary\n",
        "        self.linear_out = torch.nn.Linear(embeddings_dim, vocab_size)\n",
        "        \n",
        "        # Embeddings for priors\n",
        "        self.mu_prior = torch.nn.Embedding(vocab_size, embeddings_dim, max_norm=1.0)\n",
        "        self.sigma_prior = torch.nn.Embedding(vocab_size, embeddings_dim, max_norm=1.0)\n",
        "        \n",
        "    def reparameterize(self, mu, std):\n",
        "        # Sample from standard normal\n",
        "        eps = torch.randn_like(std)\n",
        "        \n",
        "        # Multiply by scale, add location\n",
        "        z = eps.mul(std.data).add(mu.data)\n",
        "        \n",
        "        return z\n",
        "    \n",
        "        \n",
        "    def encode(self, target, context):\n",
        "        \n",
        "        batch_size = target.size(0)\n",
        "        \n",
        "        # Lookup internal embeddings\n",
        "        target_emb = self.embed(target)\n",
        "        context_emb = self.embed(context)\n",
        "                \n",
        "        # Repeat target to number of context words\n",
        "        window_size = context.size(1)\n",
        "        \n",
        "        target_emb = target_emb.repeat(1, window_size, 1).view(batch_size, window_size, -1)\n",
        "        \n",
        "        # Concatenate and sum context\n",
        "        combined = torch.cat((target_emb, context_emb), 2)\n",
        "        combined = F.relu(combined)\n",
        "        summed = torch.sum(combined, 1)\n",
        "                                \n",
        "        # Estimate mu and sigma transformations\n",
        "        mu = self.linear_mu(summed)\n",
        "        sigma = torch.sqrt(F.softplus(self.linear_sigma(summed)))\n",
        "        \n",
        "        return mu, sigma\n",
        "        \n",
        "    def decode(self, z):\n",
        "        return F.log_softmax(self.linear_out(z), 1)\n",
        "     \n",
        "    def forward(self, target, context):\n",
        "        mu, sigma = self.encode(target, context)\n",
        "        z = Variable(self.reparameterize(mu, sigma), requires_grad = True)\n",
        "        return self.decode(z), mu, sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_00yileWTGbj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def kl_loss(mu_prior, sigma_prior, mu_data, sigma_data):\n",
        "    term1 = torch.log(sigma_prior / sigma_data)\n",
        "    term2 = (sigma_data.pow(2) + (mu_data - mu_prior).pow(2)) / (2 * sigma_prior.pow(2))\n",
        "    \n",
        "    kl = torch.sum(term1 + term2, 1) - 0.5\n",
        "    \n",
        "    return kl\n",
        "\n",
        "def bsg_score(prediction, context, mu_prior, sigma_prior, mu_data, sigma_data):\n",
        "    \n",
        "    # Take probablities from prediction matrix (reach row is one member of batch)\n",
        "    probs = Variable(torch.empty_like(context).float())\n",
        "    for i in range(prediction.size(0)):\n",
        "        probs[i, :] = prediction[i, context[i]]\n",
        "                \n",
        "    # Calculate the loss for each member of the batch\n",
        "    batch_loss = torch.sum(probs, 1) - kl_loss(mu_prior, sigma_prior, mu_data, sigma_data)\n",
        "           \n",
        "    # Average the loss\n",
        "    return torch.mean(batch_loss).squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uIomeAR9eGWv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def init_model_dir(model_name, lr, embedding_dim, window_size, batch_size, vocab_size):\n",
        "    label = \"{}_lr-{}_emb-{}_window-{}_batch-{}_vocab-{}\".format(model_name, lr, embedding_dim, window_size, batch_size, vocab_size)\n",
        "    \n",
        "    # Create directory to save model and save each epoch\n",
        "    if not os.path.exists(label):\n",
        "        os.makedirs(label)\n",
        "        print(\"First time running hyperparamter settings, created directory.\")\n",
        "            \n",
        "    return label\n",
        "\n",
        "def train(n_epochs, batch_size, embedding_dim, corpus, vocab_size, test_corpus, window_size, lr, label):\n",
        "    w2i, i2w, vocab_size = create_vocabulary(corpus, n=vocab_size)\n",
        "    model = BayesianSkipGram(vocab_size, embedding_dim)\n",
        "    obj_func = bsg_score\n",
        "    optimizer = Adam(model.parameters(), lr)\n",
        "    \n",
        "    print(vocab_size)\n",
        "    \n",
        "    # Create a directory to save model data (epochs and w2i mapping)\n",
        "    model_dir = init_model_dir(label, lr, embedding_dim, window_size, batch_size, vocab_size)\n",
        "    \n",
        "    try:\n",
        "        with open(os.path.join(model_dir, sorted(os.listdir(model_dir))[-1]), 'rb') as f:\n",
        "            w2i = pickle.load(f)\n",
        "            w2i = defaultdict(lambda: w2i['<unk>'], w2i)\n",
        "            \n",
        "        epoch_file_name = sorted(os.listdir(model_dir))[-2]\n",
        "        epoch_num = epoch_file_name.split('.')[0][3:]\n",
        "        with open(os.path.join(model_dir, epoch_file_name), 'rb') as f:       \n",
        "            model = pickle.load(f)\n",
        "            \n",
        "        print(\"Continuing from epoch {}\".format(epoch_num))\n",
        "    except Exception as e:\n",
        "        print(\"No trained model found, starting from epoch 0\")\n",
        "        print(e)\n",
        "        epoch_num = 0\n",
        "    \n",
        "    # Save corresponding w2i dict. The ordering can be different every time\n",
        "    # because vocabulary is created using sets. This way the model and ordering\n",
        "    # are always paired.\n",
        "    with open(os.path.join(model_dir, 'w2i.pickle'), 'wb') as f:\n",
        "        pickle.dump(dict(w2i), f)\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        if epoch < int(epoch_num): continue\n",
        "            \n",
        "        epoch_loss = 0\n",
        "        examples = 0\n",
        "        start = time.time()\n",
        "        \n",
        "        data = generate_bsg_data(corpus, window_size, w2i)\n",
        "        \n",
        "        while True:\n",
        "            try:\n",
        "                targets, contexts = get_bsg_batch(data, batch_size)\n",
        "            except:\n",
        "                print(\"Reached end of data\")\n",
        "                break\n",
        "                \n",
        "            examples += 1\n",
        "\n",
        "            # Forward pass\n",
        "            pred, mu, sigma = model(targets, contexts)\n",
        "            \n",
        "            # Calculate loss\n",
        "            s_prior = F.softplus(model.sigma_prior(targets).squeeze())\n",
        "            m_prior = model.mu_prior(targets).squeeze()\n",
        "            \n",
        "            loss = -obj_func(pred, contexts, m_prior, s_prior, mu, sigma)\n",
        "            epoch_loss += loss\n",
        "\n",
        "            # Do SGD step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "                        \n",
        "            if examples % 1000 == 0:\n",
        "                sys.stdout.write(\"\\r{:.2f} target number {}\".format(time.time() - start, examples * batch_size))\n",
        "                print()\n",
        "                            \n",
        "        with open(os.path.join(model_dir, 'ep-{}.pickle'.format(epoch)), 'wb') as f:\n",
        "            pickle.dump(model.cpu(), f)\n",
        "\n",
        "        print(\"{:.2f} sec: EPOCH {}/{} \\t\\t LOSS\".format(time.time() - start, epoch, n_epochs),\n",
        "              epoch_loss.item() / examples)\n",
        "        \n",
        "        if test_corpus:\n",
        "            validate_bsg(model, test_corpus, window_size, w2i)\n",
        "        \n",
        "    \n",
        "    return model_dir\n",
        "\n",
        "def validate_bsg(trained_model, corpus, window_size, w2i):\n",
        "    \n",
        "    obj_func = bsg_score\n",
        "    validation_loss = 0\n",
        "    examples = 0\n",
        "    \n",
        "    data = generate_bsg_data(corpus, window_size, w2i)\n",
        "    \n",
        "    trained_model = trained_model.cpu()\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            targets, contexts = get_bsg_batch(data, 1)\n",
        "        except:\n",
        "            print(\"Reached end of data\")\n",
        "            break\n",
        "        \n",
        "        examples += 1\n",
        "        # Forward pass\n",
        "        \n",
        "        pred, mu, sigma = trained_model(targets, contexts)\n",
        "        \n",
        "        # Calculate loss\n",
        "        s_prior = F.softplus(trained_model.sigma_prior(targets).squeeze())\n",
        "        m_prior = trained_model.mu_prior(targets).squeeze()\n",
        "        loss = -obj_func(pred, contexts, m_prior, s_prior, mu, sigma)\n",
        "        validation_loss += loss\n",
        "        \n",
        "        \n",
        "    print(\"v \\t\\t\\t\\t VALIDATION LOSS {}\".format(validation_loss.item() / examples))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tKMWYxIKd9Ou",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 3485
        },
        "outputId": "9dfb354b-73ba-4932-b119-e826917f0a91",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526567452518,
          "user_tz": -120,
          "elapsed": 24995,
          "user": {
            "displayName": "Aron Hammond",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106989373078986092985"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "dev_en_dir = train(n_epochs=100, \n",
        "                   batch_size=20, \n",
        "                   embedding_dim=100, \n",
        "                   corpus=dev_en,\n",
        "                   vocab_size=10000,\n",
        "                   test_corpus=None, \n",
        "                   window_size=6, \n",
        "                   lr=1e-3, \n",
        "                   label=\"relu-bsg-dev\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "251\n",
            "First time running hyperparamter settings, created directory.\n",
            "No trained model found, starting from epoch 0\n",
            "list index out of range\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 0/100 \t\t LOSS 89.73727213541666\n",
            "Reached end of data\n",
            "0.52 sec: EPOCH 1/100 \t\t LOSS 86.76612955729166\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 2/100 \t\t LOSS 85.81471354166666\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 3/100 \t\t LOSS 85.25535481770834\n",
            "Reached end of data\n",
            "0.22 sec: EPOCH 4/100 \t\t LOSS 84.82249348958334\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 5/100 \t\t LOSS 84.53392740885417\n",
            "Reached end of data\n",
            "0.22 sec: EPOCH 6/100 \t\t LOSS 84.17923177083334\n",
            "Reached end of data\n",
            "0.20 sec: EPOCH 7/100 \t\t LOSS 83.95336100260417\n",
            "Reached end of data\n",
            "0.21 sec: EPOCH 8/100 \t\t LOSS 83.773291015625\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 9/100 \t\t LOSS 83.63318684895833\n",
            "Reached end of data\n",
            "0.22 sec: EPOCH 10/100 \t\t LOSS 83.52569986979167\n",
            "Reached end of data\n",
            "0.22 sec: EPOCH 11/100 \t\t LOSS 83.41309407552083\n",
            "Reached end of data\n",
            "0.20 sec: EPOCH 12/100 \t\t LOSS 83.13255208333334\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 13/100 \t\t LOSS 83.05381673177084\n",
            "Reached end of data\n",
            "0.22 sec: EPOCH 14/100 \t\t LOSS 82.94959309895833\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 15/100 \t\t LOSS 83.07149251302083\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 16/100 \t\t LOSS 82.92339680989583\n",
            "Reached end of data\n",
            "0.27 sec: EPOCH 17/100 \t\t LOSS 82.87710774739584\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 18/100 \t\t LOSS 82.59976399739584\n",
            "Reached end of data\n",
            "0.26 sec: EPOCH 19/100 \t\t LOSS 82.6480224609375\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 20/100 \t\t LOSS 82.70204264322916\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 21/100 \t\t LOSS 82.470654296875\n",
            "Reached end of data\n",
            "0.21 sec: EPOCH 22/100 \t\t LOSS 82.50743815104167\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 23/100 \t\t LOSS 82.46497395833333\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 24/100 \t\t LOSS 82.30948079427084\n",
            "Reached end of data\n",
            "0.20 sec: EPOCH 25/100 \t\t LOSS 82.28556315104167\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 26/100 \t\t LOSS 82.25263671875\n",
            "Reached end of data\n",
            "0.22 sec: EPOCH 27/100 \t\t LOSS 82.05172526041666\n",
            "Reached end of data\n",
            "0.22 sec: EPOCH 28/100 \t\t LOSS 82.22797037760417\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 29/100 \t\t LOSS 82.12462565104167\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 30/100 \t\t LOSS 82.15338541666667\n",
            "Reached end of data\n",
            "0.22 sec: EPOCH 31/100 \t\t LOSS 82.19417317708333\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 32/100 \t\t LOSS 82.27914225260416\n",
            "Reached end of data\n",
            "0.21 sec: EPOCH 33/100 \t\t LOSS 82.229248046875\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 34/100 \t\t LOSS 82.29246419270834\n",
            "Reached end of data\n",
            "0.22 sec: EPOCH 35/100 \t\t LOSS 82.26913248697916\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 36/100 \t\t LOSS 82.1969482421875\n",
            "Reached end of data\n",
            "0.38 sec: EPOCH 37/100 \t\t LOSS 82.0952880859375\n",
            "Reached end of data\n",
            "0.29 sec: EPOCH 38/100 \t\t LOSS 82.057568359375\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 39/100 \t\t LOSS 82.07997233072916\n",
            "Reached end of data\n",
            "0.26 sec: EPOCH 40/100 \t\t LOSS 81.8997802734375\n",
            "Reached end of data\n",
            "0.45 sec: EPOCH 41/100 \t\t LOSS 81.7879150390625\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 42/100 \t\t LOSS 81.76243489583334\n",
            "Reached end of data\n",
            "0.21 sec: EPOCH 43/100 \t\t LOSS 81.73922526041666\n",
            "Reached end of data\n",
            "0.21 sec: EPOCH 44/100 \t\t LOSS 81.7038330078125\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 45/100 \t\t LOSS 81.6624755859375\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 46/100 \t\t LOSS 81.62930501302084\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 47/100 \t\t LOSS 81.42600911458334\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 48/100 \t\t LOSS 81.50867513020833\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 49/100 \t\t LOSS 81.43665364583333\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 50/100 \t\t LOSS 81.43622233072917\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 51/100 \t\t LOSS 81.389697265625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Reached end of data\n",
            "0.22 sec: EPOCH 52/100 \t\t LOSS 81.50603841145833\n",
            "Reached end of data\n",
            "0.22 sec: EPOCH 53/100 \t\t LOSS 81.3439453125\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 54/100 \t\t LOSS 81.40604654947917\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 55/100 \t\t LOSS 81.309765625\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 56/100 \t\t LOSS 81.01993001302084\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 57/100 \t\t LOSS 81.306103515625\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 58/100 \t\t LOSS 81.25594075520833\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 59/100 \t\t LOSS 81.10419108072917\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 60/100 \t\t LOSS 81.09396158854166\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 61/100 \t\t LOSS 80.94571126302084\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 62/100 \t\t LOSS 81.17535807291667\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 63/100 \t\t LOSS 81.024951171875\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 64/100 \t\t LOSS 81.138330078125\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 65/100 \t\t LOSS 80.9467529296875\n",
            "Reached end of data\n",
            "0.22 sec: EPOCH 66/100 \t\t LOSS 80.99400227864584\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 67/100 \t\t LOSS 80.876220703125\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 68/100 \t\t LOSS 81.0046142578125\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 69/100 \t\t LOSS 80.9901611328125\n",
            "Reached end of data\n",
            "0.26 sec: EPOCH 70/100 \t\t LOSS 80.86126302083333\n",
            "Reached end of data\n",
            "0.26 sec: EPOCH 71/100 \t\t LOSS 80.79052734375\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 72/100 \t\t LOSS 81.02303873697916\n",
            "Reached end of data\n",
            "0.29 sec: EPOCH 73/100 \t\t LOSS 80.90015462239583\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 74/100 \t\t LOSS 80.7517333984375\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 75/100 \t\t LOSS 80.90380859375\n",
            "Reached end of data\n",
            "0.32 sec: EPOCH 76/100 \t\t LOSS 80.84998372395833\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 77/100 \t\t LOSS 80.648291015625\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 78/100 \t\t LOSS 80.66109212239583\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 79/100 \t\t LOSS 80.57734375\n",
            "Reached end of data\n",
            "0.26 sec: EPOCH 80/100 \t\t LOSS 80.7028564453125\n",
            "Reached end of data\n",
            "0.26 sec: EPOCH 81/100 \t\t LOSS 80.39126790364584\n",
            "Reached end of data\n",
            "0.29 sec: EPOCH 82/100 \t\t LOSS 80.55659993489583\n",
            "Reached end of data\n",
            "0.26 sec: EPOCH 83/100 \t\t LOSS 80.4384521484375\n",
            "Reached end of data\n",
            "0.32 sec: EPOCH 84/100 \t\t LOSS 80.567333984375\n",
            "Reached end of data\n",
            "0.23 sec: EPOCH 85/100 \t\t LOSS 80.53179524739583\n",
            "Reached end of data\n",
            "0.28 sec: EPOCH 86/100 \t\t LOSS 80.41563313802084\n",
            "Reached end of data\n",
            "0.28 sec: EPOCH 87/100 \t\t LOSS 80.45659993489583\n",
            "Reached end of data\n",
            "0.26 sec: EPOCH 88/100 \t\t LOSS 80.43181966145833\n",
            "Reached end of data\n",
            "0.26 sec: EPOCH 89/100 \t\t LOSS 80.4573486328125\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 90/100 \t\t LOSS 80.39375\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 91/100 \t\t LOSS 80.4906982421875\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 92/100 \t\t LOSS 80.27303873697916\n",
            "Reached end of data\n",
            "0.24 sec: EPOCH 93/100 \t\t LOSS 80.3834716796875\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 94/100 \t\t LOSS 80.15669759114583\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 95/100 \t\t LOSS 80.34473470052083\n",
            "Reached end of data\n",
            "0.27 sec: EPOCH 96/100 \t\t LOSS 80.33731282552084\n",
            "Reached end of data\n",
            "0.26 sec: EPOCH 97/100 \t\t LOSS 80.19097493489583\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 98/100 \t\t LOSS 80.22019856770834\n",
            "Reached end of data\n",
            "0.25 sec: EPOCH 99/100 \t\t LOSS 80.2669189453125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LIZMb7p_mwFQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "jot1EcOdzZsO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def get_candidates(fileName='lst/lst.gold.candidates'):\n",
        "    candidates = {}\n",
        "    \n",
        "    with open(fileName, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            target, options = line.split('::')\n",
        "            candidates[target] = [x.strip() for x in options.split(';')]\n",
        "            \n",
        "    \n",
        "    return candidates\n",
        "\n",
        "def get_test_sentences(fileName='lst/lst_test.preprocessed'):\n",
        "    \n",
        "    with open (fileName,'r') as testFile:\n",
        "        lines = testFile.readlines()\n",
        "    \n",
        "    return [line.split() for line in lines]    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-91uCk25yyPx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def bsg_embedding(model, target, context):\n",
        "    '''model: trained BayesianSkipGram\n",
        "       target: 1x1 LongTensor\n",
        "       context: (S-1)x1 LongTensor'''\n",
        "    \n",
        "    mu, sigma = model.encode(target, context)\n",
        "    qz = MultivariateNormal(mu, torch.diag(sigma.squeeze()))\n",
        "       \n",
        "    return qz\n",
        "    \n",
        "def evaluate_bsg(model, targ_idx, candidates, context=None, w2i=None):\n",
        "    \n",
        "    scored_candidates = []\n",
        "    \n",
        "    candidate_lookup = torch.LongTensor([w2i[x] for x in candidates]).unsqueeze(0)\n",
        "    target, context = make_window_vectors(context, targ_idx, 1, w2i)\n",
        "    target = target.unsqueeze(0)\n",
        "    context = context.unsqueeze(0)\n",
        "    \n",
        "    qz_target = bsg_embedding(model, target, context)\n",
        "    \n",
        "    for i in range(len(candidates)):\n",
        "        \n",
        "        qz_cand = bsg_embedding(model, candidate_lookup[0, i].unsqueeze(0), context)\n",
        "        \n",
        "        score = kl_divergence(qz_target, qz_cand)\n",
        "        scored_candidates.append((candidates[i], score.data[0].item()))\n",
        "        \n",
        "    scored_candidates = sorted(scored_candidates, key = lambda x: x[1])\n",
        "    return scored_candidates\n",
        "        \n",
        "def run_lst(model, w2i, eval_func):\n",
        "    \n",
        "    # Load evaluation data\n",
        "    sentences = get_test_sentences('lst/lst_test.preprocessed')\n",
        "    all_candidates = get_candidates('lst/lst.gold.candidates')\n",
        "    \n",
        "    result = []\n",
        "    \n",
        "    for sentence in sentences:        \n",
        "        # Encode the target withouth the POS-tag\n",
        "        target = sentence[0]\n",
        "        target_stripped = target[:-2]\n",
        "        \n",
        "        # Extract id's\n",
        "        sent_id = sentence[1]\n",
        "        targ_idx = int(sentence[2])\n",
        "        \n",
        "        # Remove target word from sentence to obtain context\n",
        "        context = sentence[3:].copy()\n",
        "        \n",
        "        scores = eval_func(model, targ_idx, all_candidates[target], context, w2i)\n",
        "        result.append(format_ranking_string(target, sent_id, scores))\n",
        "    \n",
        "    # save output to file\n",
        "    with open(os.path.join('lst', 'lst.out'), 'w') as f:\n",
        "        f.writelines(result)\n",
        "        \n",
        "    # call evaluation scripts\n",
        "    subprocess.Popen('python lst/lst_gap.py lst/lst_test.gold lst/lst.out out no-mwe'.split())\n",
        "        \n",
        "def format_ranking_string(target, sentence_id, scores):\n",
        "        \n",
        "    base = \"RANKED\\t{} {}\".format(target, sentence_id)\n",
        "    candidates = \"\".join(\"\\t{} {}\".format(x[0], x[1]) for x in scores)\n",
        "    \n",
        "    return base + candidates + \"\\n\"\n",
        "        \n",
        "def load_model(model_dir, model_file):\n",
        "    '''model_dir: path of DIRECTORY were model is stored,\n",
        "       model_file: filename of epoch you want to load'''\n",
        "    \n",
        "    with open(os.path.join(model_dir, 'w2i.pickle'), 'rb') as f:\n",
        "        w2i = pickle.load(f)\n",
        "        w2i = defaultdict(lambda: w2i['<unk>'], w2i)\n",
        "    \n",
        "    with open(os.path.join(model_dir, model_file), 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "        \n",
        "    return model, w2i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Q-XGiaJ0vKW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# RUN LST ON TRAINED BSG\n",
        "model_dir = os.path.join('bsg-dev_lr-0.001_emb-100_window-6_batch-20_vocab-251')\n",
        "trained_model, w2i = load_model(model_dir, 'ep-99.pickle')\n",
        "i2w = {i : w for (w, i) in w2i.items()}\n",
        "run_lst(trained_model, w2i, evaluate_bsg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hnGHTvcMOYNW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3f9ca456-92ef-4cf8-e6d9-09f7cba2fcde",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526567465996,
          "user_tz": -120,
          "elapsed": 77,
          "user": {
            "displayName": "Aron Hammond",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106989373078986092985"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "'''Rank the (context agnositc) embeddings of the trained model on proximity to\n",
        "the first entry'''\n",
        "\n",
        "street = torch.LongTensor([0])\n",
        "embs = trained_model.mu_prior.weight\n",
        "sims = []\n",
        "\n",
        "for i, emb in enumerate(embs):\n",
        "    word = torch.LongTensor([w2i[i]])\n",
        "    sim = F.cosine_similarity(embs[street], emb.unsqueeze(0))\n",
        "    sims.append((i2w[i], sim.item()))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('little', 1.0), ('feeling', 0.30543777346611023), ('mechanism', 0.2735344469547272), ('simply', 0.2567277252674103), ('work', 0.2533694803714752), ('equity', 0.24713867902755737), ('ought', 0.23902331292629242), ('link', 0.22124700248241425), ('fee', 0.22064056992530823), ('particular', 0.2205219566822052), ('wanted', 0.21807803213596344), ('around', 0.21661607921123505), ('conclude', 0.21308422088623047), ('unlawful', 0.19297336041927338), ('look', 0.19210302829742432), ('i', 0.19145900011062622), ('prison', 0.18539828062057495), ('28', 0.18450623750686646), ('understandably', 0.17495976388454437), ('became', 0.17223027348518372), ('purposes', 0.17209811508655548), ('speaker', 0.17059090733528137), ('hard', 0.16529786586761475), ('effective', 0.16327127814292908), ('responsible', 0.16303321719169617), ('cdc', 0.16204969584941864), ('services', 0.16060471534729004), ('within', 0.15602844953536987), ('creation', 0.1512129306793213), ('minister', 0.15026213228702545), ('something', 0.1473769098520279), ('ghettos', 0.1472049355506897), ('500', 0.14232811331748962), ('creative', 0.13925054669380188), ('workers', 0.1386110931634903), ('education', 0.1375509649515152), ('imposed', 0.13735008239746094), ('really', 0.1322564333677292), ('word', 0.12816506624221802), ('across', 0.12792517244815826), ('never', 0.12595127522945404), ('earning', 0.12506867945194244), ('issue', 0.12090925127267838), ('count', 0.11941973865032196), ('fine', 0.11871309578418732), ('enforcement', 0.11867309361696243), ('announced', 0.1181127056479454), ('canada', 0.11780209094285965), ('employment', 0.11510756611824036), ('confined', 0.11252449452877045), ('still', 0.1122407540678978), ('taxi', 0.11191578954458237), ('years', 0.11084430664777756), ('native', 0.10686859488487244), ('putting', 0.10612982511520386), ('today', 0.1054379940032959), ('inadequate', 0.10512298345565796), ('ruling', 0.10393983870744705), ('act', 0.10393255949020386), ('bail', 0.1023389995098114), ('determine', 0.10082033276557922), ('prostitutes', 0.10076451301574707), ('programs', 0.1006787121295929), ('house', 0.09834713488817215), ('justification', 0.0959743857383728), ('offer', 0.09587077051401138), ('stigmatized', 0.09486071765422821), ('complex', 0.09460069239139557), ('forthcoming', 0.09379689395427704), ('involved', 0.09226740151643753), ('entertainment', 0.09201110899448395), ('results', 0.09166493266820908), ('governments', 0.09130322933197021), ('two', 0.08910419046878815), ('example', 0.08778538554906845), ('principles', 0.08748625218868256), ('standing', 0.08695875108242035), ('husband', 0.08669993281364441), ('introduce', 0.07976266741752625), ('given', 0.0792979970574379), ('toys', 0.07787802815437317), ('even', 0.07695255428552628), ('mobilized', 0.07638996094465256), ('times', 0.07636010646820068), ('board', 0.07515906542539597), ('institute', 0.07323434203863144), ('action', 0.0708618089556694), ('commissionners', 0.07069849967956543), ('licence', 0.06918402016162872), ('another', 0.06587102264165878), ('bit', 0.06301574409008026), ('society', 0.05956738069653511), ('pertinent', 0.05901947245001793), ('voluntary', 0.058423962444067), ('want', 0.05700841546058655), ('soon', 0.05374560132622719), ('could', 0.053473033010959625), ('<unk>', 0.05323530361056328), ('<unk>', 0.05323530361056328), ('makes', 0.052770402282476425), ('going', 0.05180560052394867), ('cut', 0.0495653972029686), ('say', 0.04757247865200043), ('followed', 0.04383954405784607), ('intents', 0.04308643192052841), ('system', 0.042228955775499344), ('situation', 0.04185280576348305), ('punished', 0.04034024477005005), ('applied', 0.039857856929302216), ('budget', 0.03957882523536682), ('community', 0.038610637187957764), ('earn', 0.03609918802976608), ('weaker', 0.03176240622997284), ('person', 0.030132634565234184), ('begin', 0.029942555353045464), ('lost', 0.029476551339030266), ('made', 0.028642768040299416), ('selling', 0.02730601094663143), ('need', 0.023011714220046997), ('laws', 0.022239437326788902), ('public', 0.020281195640563965), ('spend', 0.020052848383784294), ('confusing', 0.018495354801416397), ('see', 0.017910949885845184), ('pretends', 0.015818575397133827), ('mulroney', 0.015497922897338867), ('women', 0.013746548444032669), ('way', 0.012429247610270977), ('treasury', 0.012065366841852665), ('passenger', 0.011166737414896488), ('clear', 0.011145580559968948), ('aggressive', 0.010816973634064198), ('context', 0.010683617554605007), ('yesterday', 0.01043467503041029), ('streets', 0.010163901373744011), ('meaningful', 0.009716846980154514), ('pressure', 0.007753122132271528), ('last', 0.007549412082880735), ('c', 0.0074188364669680595), ('saw', 0.006980761419981718), ('suffer', 0.004746389575302601), ('british', 0.004666944034397602), ('hooker', 0.0040506585501134396), ('one', 0.002658810932189226), ('activity', 0.002366892760619521), ('questions', 0.0022794234100729227), ('limits', 0.002176041482016444), ('gives', 0.0013239674735814333), ('poor', -0.00045317562762647867), ('june', -0.002484000287950039), ('rehabilitation', -0.004001534078270197), ('interest', -0.005857350304722786), ('driver', -0.007554388139396906), ('country', -0.009431988932192326), ('legal', -0.009840142913162708), ('making', -0.010135803371667862), ('considerable', -0.015285586938261986), ('sanctions', -0.01712445728480816), ('let', -0.019486792385578156), ('affirmative', -0.020040597766637802), ('ensuring', -0.020147856324911118), ('noted', -0.020997708663344383), ('sex', -0.02127239853143692), ('establishment', -0.021777154877781868), ('safe', -0.022378087043762207), ('presented', -0.023229485377669334), ('prime', -0.02334517054259777), ('cost', -0.02505525015294552), ('constrained', -0.025108927860856056), ('raises', -0.02674310840666294), ('ownership', -0.02714766189455986), ('growers', -0.03203470632433891), ('blessings', -0.033389005810022354), ('ask', -0.03377935290336609), ('obviously', -0.033880118280649185), ('ground', -0.035684023052453995), ('research', -0.03587217256426811), ('1985', -0.037206102162599564), ('method', -0.03824026510119438), ('violate', -0.0390293151140213), ('freedom', -0.04292307421565056), ('<s>', -0.04431770741939545), ('mr.', -0.04695688188076019), ('stressed', -0.047124095261096954), ('contact', -0.04972432926297188), ('responsibilities', -0.05251815915107727), ('think', -0.05307557433843613), ('come', -0.05404061824083328), ('wife', -0.05408831313252449), ('order', -0.055826105177402496), ('federal', -0.05615878477692604), ('stayed', -0.05753275752067566), ('easy', -0.05843021720647812), ('good', -0.06152033805847168), ('mean', -0.06228712573647499), ('grants', -0.06496818363666534), ('raise', -0.06522638350725174), ('went', -0.06785199046134949), ('250,000', -0.07053534686565399), ('backs', -0.07054465264081955), ('</s>', -0.07072573900222778), ('committee', -0.07241416722536087), ('49', -0.07242266833782196), ('corporations', -0.07264119386672974), ('bad', -0.07314269244670868), ('take', -0.07474078983068466), ('whether', -0.07570427656173706), ('essentially', -0.07812231779098511), ('aware', -0.08023209869861603), ('course', -0.0852731242775917), ('incomes', -0.0867655873298645), ('supply', -0.08832598477602005), ('process', -0.09009744226932526), ('consent', -0.09410427510738373), ('important', -0.09537910670042038), ('cabinet', -0.09749245643615723), ('political', -0.09853115677833557), ('terms', -0.10655776411294937), ('unanimous', -0.10669806599617004), ('tax', -0.11183793097734451), ('met', -0.11549156904220581), ('35,000', -0.11763756722211838), ('support', -0.11924457550048828), ('whole', -0.12358453869819641), ('already', -0.12654924392700195), ('columbia', -0.12756650149822235), ('buying', -0.1330500692129135), ('job', -0.1333901733160019), ('government', -0.13492026925086975), ('rather', -0.13945196568965912), ('however', -0.13970883190631866), ('prostitute', -0.1596541553735733), ('spring', -0.16043148934841156), ('family', -0.16439932584762573), ('well', -0.16813203692436218), ('money', -0.16821548342704773), ('much', -0.17568226158618927), ('us', -0.17874085903167725), ('show', -0.1790824830532074), ('25', -0.1795770227909088), ('make', -0.18951347470283508), ('revenue', -0.2141723334789276), ('bill', -0.2158505767583847), ('street', -0.21828150749206543), ('far', -0.22124961018562317), ('issued', -0.23399227857589722), ('vote', -0.24456562101840973), ('break', -0.2520352005958557), ('pay', -0.269369900226593), ('ensure', -0.2760147452354431), ('would', -0.2798160910606384), ('people', -0.2835734784603119)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}