{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Google not needed on local runtime\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import scipy.spatial.distance as sd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import os\n",
    "import string\n",
    "import io\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "import subprocess\n",
    "import cProfile\n",
    "import operator    \n",
    "import nltk\n",
    "import sys\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except:\n",
    "    print(\"Google not needed on local runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.kl import kl_divergence\n",
    "import torch.optim as optim\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file):\n",
    "    '''Read text in file and tokenize sentences'''\n",
    "    sentences = []\n",
    "\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    with open(file) as f:\n",
    "        for line in f.readlines():\n",
    "            tokens = line.split()\n",
    "            tokens = [token.lower() for token in tokens if token not in stopWords]\n",
    "            tokens = list(filter(lambda x: x not in string.punctuation, tokens))\n",
    "            sentences.append(tokens)\n",
    "\n",
    "    return sentences            \n",
    "\n",
    "def create_vocabulary(corpus, n=30000):\n",
    "    \n",
    "    all_words = list(itertools.chain(*corpus))\n",
    "    top_n = set([i[0] for i in Counter(all_words).most_common(n)])\n",
    "    vocabulary = set(all_words).intersection(top_n)\n",
    " \n",
    "    # Placeholder for unknown words\n",
    "    vocabulary.add(\"<unk>\")\n",
    "\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "\n",
    "    for (idx, word) in enumerate(list(vocabulary)):\n",
    "        word2idx[word] = idx\n",
    "        idx2word[idx] = word\n",
    "\n",
    "    # Return the ID for <unk> for new words\n",
    "    word2idx = defaultdict(lambda: word2idx[\"<unk>\"], word2idx)\n",
    "\n",
    "    vocabulary_size = len(vocabulary)\n",
    "\n",
    "    return word2idx, idx2word, vocabulary_size\n",
    "\n",
    "def create_center_context_pairs(corpus,window_size,word2idx):\n",
    "    \n",
    "    num_sentences = len(corpus)\n",
    "    i = 0.0\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        idx = [word2idx[word] for word in sentence]\n",
    "        i += 1        \n",
    "        \n",
    "        for center_word_pos in range(0,len(sentence)):\n",
    "            center_word = sentence[center_word_pos]\n",
    "            \n",
    "            for context_word_pos in range(-window_size+center_word_pos, window_size + center_word_pos + 1):\n",
    "                \n",
    "                if center_word_pos == context_word_pos or context_word_pos < 0 or context_word_pos >= len(sentence):\n",
    "                    continue\n",
    "                \n",
    "                context_word = sentence[context_word_pos]\n",
    "                 \n",
    "                # Determine indices which to remove from sentence for negative sampling\n",
    "                mi = max(0,center_word_pos-window_size)\n",
    "                ma = min(len(sentence),center_word_pos+1+window_size)\n",
    "                \n",
    "                # Make a copy of the sentence for manipulation\n",
    "                negative_sample_sent = copy.copy(sentence)\n",
    "                indexes = [i for i in range(mi,ma)]\n",
    "                \n",
    "                # Make sure to remove double words\n",
    "                indexes.append(center_word_pos)\n",
    "                indexes = list(set(indexes))\n",
    "\n",
    "                # Delete the words\n",
    "                for index in sorted(indexes, reverse=True):\n",
    "                    del negative_sample_sent[index]\n",
    "                \n",
    "                # If we can not determine a negative sample, skip the pair altogether\n",
    "                try:\n",
    "                    neg_sample = random.choice(negative_sample_sent)\n",
    "                    pair = (idx[center_word_pos], idx[context_word_pos],word2idx[neg_sample])\n",
    "                    assert pair[1] != pair[2]\n",
    "                    assert pair[0] != pair[2]\n",
    "                    \n",
    "                    pairs.append(pair)\n",
    "                    \n",
    "                except:\n",
    "                    continue\n",
    "       \n",
    "    return np.array(pairs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim):\n",
    "        super(SkipGram,self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.W1 = torch.nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.W2 = torch.nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        \n",
    "        initrange = 0.5 / self.embedding_dim\n",
    "        self.W1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.W2.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        \n",
    "    def forward_batch(self, batch_c,batch_t,batch_n):\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        center = Variable(torch.LongTensor(batch_c))\n",
    "        target = Variable(torch.LongTensor(batch_t))\n",
    "        negative = Variable(torch.LongTensor(batch_n))\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            center = center.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "        batch_size = len(center)\n",
    "                        \n",
    "        emb_w1 = self.W1(center)\n",
    "        emb_w2 = self.W2(target)\n",
    "        \n",
    "        z2 = torch.mul(emb_w1, emb_w2)        \n",
    "        \n",
    "        score = torch.sum(z2, dim=1)\n",
    "        score = F.logsigmoid(score).squeeze()  \n",
    "        \n",
    "        losses.append(sum(score))\n",
    "        \n",
    "        neg_emb_w2 = self.W2(negative)\n",
    "        \n",
    "        neg_score = torch.mul(neg_emb_w2,emb_w1)\n",
    "        neg_score = torch.sum(neg_score, dim=1)\n",
    "        neg_score = F.logsigmoid(-1 * neg_score).squeeze()  \n",
    "        \n",
    "        loss = score + neg_score\n",
    "        \n",
    "        losses.append(sum(neg_score))\n",
    "\n",
    "        return -1 * loss.sum()/batch_size\n",
    "    \n",
    "    def save_embedding(self, idx2word, word2idx, file_name):\n",
    "        \n",
    "        word2embedW1 = {}\n",
    "        word2embedW2 = {}\n",
    "        \n",
    "        for key, value in word2idx.items():\n",
    "            idx = word2idx[key]\n",
    "            word2embedW1[key] = self.W1.weight.data[idx].numpy()\n",
    "            word2embedW2[key] = self.W2.weight.data[idx].numpy()\n",
    "        \n",
    "        with open('SG/idx2word.pickle','wb') as f:\n",
    "            pickle.dump(dict(idx2word),f)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        with open('SG/word2idx.pickle','wb') as f:\n",
    "            pickle.dump(dict(word2idx),f)\n",
    "            \n",
    "        time.sleep(1)\n",
    "        \n",
    "        with open('SG/word2embed_W1.pickle','wb') as f:\n",
    "            pickle.dump(word2embedW1,f)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        with open('SG/word2embed_W2.pickle','wb') as f:\n",
    "            pickle.dump(word2embedW2,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V:\n",
    "    \n",
    "    def __init__(self, input_data, embedding_dim, vocab_size, window_size,epochs):\n",
    "        \n",
    "        self.input_data = input_data\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.window_size = window_size\n",
    "        self.epochs = epochs\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    def get_batches(self,data,batch_size,vocab_size,window_size):\n",
    "        \n",
    "        center = data[:,0]\n",
    "        target = data[:,1]\n",
    "        neg_sample = data[:,2]\n",
    "        \n",
    "        num_pairs = data.shape[0]\n",
    "        \n",
    "        batches_c = []\n",
    "        batches_t = []\n",
    "        batches_n = []\n",
    "        \n",
    "        batch_i_c = []\n",
    "        batch_i_t = []\n",
    "        batch_i_n = []\n",
    "        \n",
    "        for i in range(0,num_pairs):\n",
    "            batch_i_c.append(center[i])\n",
    "            batch_i_t.append(target[i])   \n",
    "            batch_i_n.append(neg_sample[i])\n",
    "            \n",
    "            if len(batch_i_c) % batch_size == 0:\n",
    "                batch_i_c = np.stack(batch_i_c)\n",
    "                batch_i_t = np.stack(batch_i_t)\n",
    "                batch_i_n = np.stack(batch_i_n)\n",
    "                \n",
    "                batches_c.append(batch_i_c)\n",
    "                batches_t.append(batch_i_t)\n",
    "                batches_n.append(batch_i_n)\n",
    "                \n",
    "                batch_i_c = []\n",
    "                batch_i_t = []        \n",
    "                batch_i_n = []\n",
    "        \n",
    "        return batches_c, batches_t,batches_n\n",
    "        \n",
    "    def train_batch(self,idx2word,word2idx,batch_size=50,verbose=False):\n",
    "        \n",
    "        model = SkipGram(self.vocab_size, self.embedding_dim) \n",
    "        \n",
    "        if self.use_cuda:\n",
    "            model.cuda()\n",
    "            \n",
    "        optimizer = optim.SGD(model.parameters(),lr=0.2)\n",
    "\n",
    "        batches_c,batches_t,batches_n = self.get_batches(self.input_data,batch_size,self.vocab_size,self.window_size)\n",
    "        num_batches = len(batches_c)\n",
    "        \n",
    "        print ('There are',num_batches,'batches')\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(0,self.epochs):            \n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "            for i in range(0,len(batches_c)):\n",
    "                model.zero_grad()\n",
    "                loss = model.forward_batch(batches_c[i],batches_t[i],batches_n[i])\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                                    \n",
    "                optimizer.step()\n",
    "                \n",
    "                if verbose and i%20000 == 0 and i > 1:\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print ('Iteration',i,'of',num_batches,'  ETR:',(elapsed_time/i)*(num_batches-i))\n",
    "\n",
    "            print ('After epoch',epoch,'the loss is',epoch_loss)\n",
    "                \n",
    "        model.save_embedding(idx2word,word2idx,'skipgram.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size is 18001\n",
      "There are 5661429 pairs\n",
      "Starting to train\n",
      "There are 113228 batches\n",
      "Iteration 20000 of 113228   ETR: 763.8201275562286\n",
      "Iteration 40000 of 113228   ETR: 597.6539674914361\n",
      "Iteration 60000 of 113228   ETR: 433.84924714196524\n",
      "Iteration 80000 of 113228   ETR: 270.90632264277934\n",
      "Iteration 100000 of 113228   ETR: 107.56040503824232\n",
      "After epoch 0 the loss is 154222.1702940166\n",
      "Iteration 20000 of 113228   ETR: 757.7441552088261\n",
      "Iteration 40000 of 113228   ETR: 595.5668396669388\n",
      "Iteration 60000 of 113228   ETR: 434.23932781833014\n",
      "Iteration 80000 of 113228   ETR: 271.4837219695687\n",
      "Iteration 100000 of 113228   ETR: 107.87669907465934\n",
      "After epoch 1 the loss is 148779.72818940878\n",
      "Iteration 20000 of 113228   ETR: 762.7756984445572\n",
      "Iteration 40000 of 113228   ETR: 599.5847646928787\n",
      "Iteration 60000 of 113228   ETR: 434.8998329916001\n",
      "Iteration 80000 of 113228   ETR: 271.9720429028392\n",
      "Iteration 100000 of 113228   ETR: 110.97147441476822\n",
      "After epoch 2 the loss is 145461.30143372715\n",
      "Iteration 20000 of 113228   ETR: 756.2859529191971\n",
      "Iteration 40000 of 113228   ETR: 598.3298558047534\n",
      "Iteration 60000 of 113228   ETR: 433.377831922245\n",
      "Iteration 80000 of 113228   ETR: 272.1944113543153\n",
      "Iteration 100000 of 113228   ETR: 108.10714879341124\n",
      "After epoch 3 the loss is 142931.13365760446\n",
      "Iteration 20000 of 113228   ETR: 754.6958894704343\n",
      "Iteration 40000 of 113228   ETR: 597.3041658939362\n",
      "Iteration 60000 of 113228   ETR: 433.4048788559278\n",
      "Iteration 80000 of 113228   ETR: 270.4895975516796\n",
      "Iteration 100000 of 113228   ETR: 107.86374672947883\n",
      "After epoch 4 the loss is 140843.4835659191\n",
      "Iteration 20000 of 113228   ETR: 760.4876870611668\n",
      "Iteration 40000 of 113228   ETR: 595.9560722255707\n",
      "Iteration 60000 of 113228   ETR: 434.6631668226401\n",
      "Iteration 80000 of 113228   ETR: 270.9666874189854\n",
      "Iteration 100000 of 113228   ETR: 107.76442369725228\n",
      "After epoch 5 the loss is 139039.15707932413\n",
      "Iteration 20000 of 113228   ETR: 757.8890004378319\n",
      "Iteration 40000 of 113228   ETR: 597.343206648302\n",
      "Iteration 60000 of 113228   ETR: 435.48832778730394\n",
      "Iteration 80000 of 113228   ETR: 271.7553184051394\n",
      "Iteration 100000 of 113228   ETR: 108.0374527251339\n",
      "After epoch 6 the loss is 137437.88715699315\n",
      "Iteration 20000 of 113228   ETR: 767.5358120501518\n",
      "Iteration 40000 of 113228   ETR: 597.9641792516708\n",
      "Iteration 60000 of 113228   ETR: 433.8385712217172\n",
      "Iteration 80000 of 113228   ETR: 271.1078521164536\n",
      "Iteration 100000 of 113228   ETR: 107.999974625988\n",
      "After epoch 7 the loss is 135982.98483306915\n",
      "Iteration 20000 of 113228   ETR: 759.1977275545121\n",
      "Iteration 40000 of 113228   ETR: 595.4677956745862\n",
      "Iteration 60000 of 113228   ETR: 432.6662143134435\n",
      "Iteration 80000 of 113228   ETR: 270.5814514783502\n",
      "Iteration 100000 of 113228   ETR: 107.73450109600067\n",
      "After epoch 8 the loss is 134630.98827210814\n",
      "Iteration 20000 of 113228   ETR: 756.976849246931\n",
      "Iteration 40000 of 113228   ETR: 595.8107324264288\n",
      "Iteration 60000 of 113228   ETR: 436.3472257174969\n",
      "Iteration 80000 of 113228   ETR: 271.64707687039373\n",
      "Iteration 100000 of 113228   ETR: 107.96493170271874\n",
      "After epoch 9 the loss is 133350.28365261108\n",
      "Iteration 20000 of 113228   ETR: 763.3106847996712\n",
      "Iteration 40000 of 113228   ETR: 597.1565555628061\n",
      "Iteration 60000 of 113228   ETR: 443.1277916941643\n",
      "Iteration 80000 of 113228   ETR: 280.66874744392635\n",
      "Iteration 100000 of 113228   ETR: 110.8494129191208\n",
      "After epoch 10 the loss is 132118.89191879332\n",
      "Iteration 20000 of 113228   ETR: 761.2682060339928\n",
      "Iteration 40000 of 113228   ETR: 599.4858734660387\n",
      "Iteration 60000 of 113228   ETR: 434.6577999915282\n",
      "Iteration 80000 of 113228   ETR: 271.0303918860555\n",
      "Iteration 100000 of 113228   ETR: 107.82954170804976\n",
      "After epoch 11 the loss is 130922.49009612575\n",
      "Iteration 20000 of 113228   ETR: 755.5452474633694\n",
      "Iteration 40000 of 113228   ETR: 594.4295462562799\n",
      "Iteration 60000 of 113228   ETR: 432.7067925398032\n",
      "Iteration 80000 of 113228   ETR: 270.0000320451021\n",
      "Iteration 100000 of 113228   ETR: 107.52341183599472\n",
      "After epoch 12 the loss is 129751.54090427235\n",
      "Iteration 20000 of 113228   ETR: 777.5867413068294\n",
      "Iteration 40000 of 113228   ETR: 605.1219719839811\n",
      "Iteration 60000 of 113228   ETR: 380.18534133377074\n",
      "Iteration 80000 of 113228   ETR: 188.14060031772854\n",
      "Iteration 100000 of 113228   ETR: 63.09354262525559\n",
      "After epoch 13 the loss is 128599.61738770828\n",
      "Iteration 20000 of 113228   ETR: 112.75139669566154\n",
      "Iteration 40000 of 113228   ETR: 88.41990867738724\n",
      "Iteration 60000 of 113228   ETR: 64.63183928378423\n",
      "Iteration 80000 of 113228   ETR: 40.24185020853281\n",
      "Iteration 100000 of 113228   ETR: 16.006239848690033\n",
      "After epoch 14 the loss is 127462.39553555846\n",
      "Iteration 20000 of 113228   ETR: 112.37749593701363\n",
      "Iteration 40000 of 113228   ETR: 88.7503116736412\n",
      "Iteration 60000 of 113228   ETR: 64.48862924291294\n",
      "Iteration 80000 of 113228   ETR: 40.264338879740244\n",
      "Iteration 100000 of 113228   ETR: 16.029981126203538\n",
      "After epoch 15 the loss is 126336.72689311951\n",
      "Iteration 20000 of 113228   ETR: 113.55061659665107\n",
      "Iteration 40000 of 113228   ETR: 89.0752076914072\n",
      "Iteration 60000 of 113228   ETR: 64.62205149011612\n",
      "Iteration 80000 of 113228   ETR: 40.35304295060635\n",
      "Iteration 100000 of 113228   ETR: 16.05120226444244\n",
      "After epoch 16 the loss is 125220.24412132427\n",
      "Iteration 20000 of 113228   ETR: 116.9858760772705\n",
      "Iteration 40000 of 113228   ETR: 90.67400265991688\n",
      "Iteration 60000 of 113228   ETR: 65.61473229047458\n",
      "Iteration 80000 of 113228   ETR: 40.89985169935227\n",
      "Iteration 100000 of 113228   ETR: 16.27296427075386\n",
      "After epoch 17 the loss is 124111.3706047386\n",
      "Iteration 20000 of 113228   ETR: 113.36832839083672\n",
      "Iteration 40000 of 113228   ETR: 89.61854180603027\n",
      "Iteration 60000 of 113228   ETR: 65.06591025080681\n",
      "Iteration 80000 of 113228   ETR: 40.60250434094668\n",
      "Iteration 100000 of 113228   ETR: 16.28516623207092\n",
      "After epoch 18 the loss is 123009.29704992473\n",
      "Iteration 20000 of 113228   ETR: 113.66278326787949\n",
      "Iteration 40000 of 113228   ETR: 89.11375785024165\n",
      "Iteration 60000 of 113228   ETR: 64.85243266709647\n",
      "Iteration 80000 of 113228   ETR: 40.567935049796105\n",
      "Iteration 100000 of 113228   ETR: 16.149748420877454\n",
      "After epoch 19 the loss is 121913.86058458686\n",
      "Iteration 20000 of 113228   ETR: 114.71439735016823\n",
      "Iteration 40000 of 113228   ETR: 89.66626793448926\n",
      "Iteration 60000 of 113228   ETR: 65.29184381508827\n",
      "Iteration 80000 of 113228   ETR: 40.75450845197439\n",
      "Iteration 100000 of 113228   ETR: 16.21646234957695\n",
      "After epoch 20 the loss is 120825.40390051901\n",
      "Iteration 20000 of 113228   ETR: 113.2234487095356\n",
      "Iteration 40000 of 113228   ETR: 88.77233653228284\n",
      "Iteration 60000 of 113228   ETR: 64.58647270361583\n",
      "Iteration 80000 of 113228   ETR: 40.2260242824912\n",
      "Iteration 100000 of 113228   ETR: 15.995363776836394\n",
      "After epoch 21 the loss is 119744.6107023675\n",
      "Iteration 20000 of 113228   ETR: 112.59248048243523\n",
      "Iteration 40000 of 113228   ETR: 88.85310191283226\n",
      "Iteration 60000 of 113228   ETR: 64.36231814211209\n",
      "Iteration 80000 of 113228   ETR: 40.189791929268836\n",
      "Iteration 100000 of 113228   ETR: 16.039662948246004\n",
      "After epoch 22 the loss is 118672.3607031256\n",
      "Iteration 20000 of 113228   ETR: 113.20008894195557\n",
      "Iteration 40000 of 113228   ETR: 88.56859447772504\n",
      "Iteration 60000 of 113228   ETR: 65.9189419769764\n",
      "Iteration 80000 of 113228   ETR: 41.806896091747284\n",
      "Iteration 100000 of 113228   ETR: 16.499899046792983\n",
      "After epoch 23 the loss is 117609.66651123948\n",
      "Iteration 20000 of 113228   ETR: 111.93346475286485\n",
      "Iteration 40000 of 113228   ETR: 88.11703664531707\n",
      "Iteration 60000 of 113228   ETR: 63.93446871434847\n",
      "Iteration 80000 of 113228   ETR: 39.95490981209279\n",
      "Iteration 100000 of 113228   ETR: 15.922813613300322\n",
      "After epoch 24 the loss is 116557.67429540306\n",
      "Iteration 20000 of 113228   ETR: 112.58780719528198\n",
      "Iteration 40000 of 113228   ETR: 88.37585372242928\n",
      "Iteration 60000 of 113228   ETR: 64.24490670839945\n",
      "Iteration 80000 of 113228   ETR: 40.10358621454239\n",
      "Iteration 100000 of 113228   ETR: 16.002791482715605\n",
      "After epoch 25 the loss is 115517.65419867262\n",
      "Iteration 20000 of 113228   ETR: 112.48031047711372\n",
      "Iteration 40000 of 113228   ETR: 88.33180138630867\n",
      "Iteration 60000 of 113228   ETR: 64.1337172369639\n",
      "Iteration 80000 of 113228   ETR: 40.1294065554142\n",
      "Iteration 100000 of 113228   ETR: 15.963001647462844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 26 the loss is 114490.94148848392\n",
      "Iteration 20000 of 113228   ETR: 112.50367913560868\n",
      "Iteration 40000 of 113228   ETR: 88.19229330148697\n",
      "Iteration 60000 of 113228   ETR: 64.38366554091772\n",
      "Iteration 80000 of 113228   ETR: 40.19645546665192\n",
      "Iteration 100000 of 113228   ETR: 15.999342832899094\n",
      "After epoch 27 the loss is 113478.85808424279\n",
      "Iteration 20000 of 113228   ETR: 111.6857649746418\n",
      "Iteration 40000 of 113228   ETR: 87.7719528833151\n",
      "Iteration 60000 of 113228   ETR: 64.0910228623708\n",
      "Iteration 80000 of 113228   ETR: 40.020710981154444\n",
      "Iteration 100000 of 113228   ETR: 15.926925665149689\n",
      "After epoch 28 the loss is 112482.6534333229\n",
      "Iteration 20000 of 113228   ETR: 112.61586025457382\n",
      "Iteration 40000 of 113228   ETR: 88.86594643712044\n",
      "Iteration 60000 of 113228   ETR: 64.34630479051272\n",
      "Iteration 80000 of 113228   ETR: 40.2901570420146\n",
      "Iteration 100000 of 113228   ETR: 16.020563655757904\n",
      "After epoch 29 the loss is 111503.484378919\n",
      "Iteration 20000 of 113228   ETR: 113.04585379543305\n",
      "Iteration 40000 of 113228   ETR: 88.53554955148697\n",
      "Iteration 60000 of 113228   ETR: 64.35520361156463\n",
      "Iteration 80000 of 113228   ETR: 40.06402268679142\n",
      "Iteration 100000 of 113228   ETR: 15.94217848736763\n",
      "After epoch 30 the loss is 110542.42110044137\n",
      "Iteration 20000 of 113228   ETR: 112.71868813104629\n",
      "Iteration 40000 of 113228   ETR: 88.12805409407615\n",
      "Iteration 60000 of 113228   ETR: 63.88021283103625\n",
      "Iteration 80000 of 113228   ETR: 39.872452077198034\n",
      "Iteration 100000 of 113228   ETR: 15.896818251161575\n",
      "After epoch 31 the loss is 109600.45846527256\n",
      "Iteration 20000 of 113228   ETR: 111.96620221295356\n",
      "Iteration 40000 of 113228   ETR: 87.76093805339337\n",
      "Iteration 60000 of 113228   ETR: 63.885549627860385\n",
      "Iteration 80000 of 113228   ETR: 39.88702828052043\n",
      "Iteration 100000 of 113228   ETR: 15.917906519765856\n",
      "After epoch 32 the loss is 108678.52420717664\n",
      "Iteration 20000 of 113228   ETR: 115.16308626236915\n",
      "Iteration 40000 of 113228   ETR: 89.44049185535908\n",
      "Iteration 60000 of 113228   ETR: 64.67364489727021\n",
      "Iteration 80000 of 113228   ETR: 40.33763442403078\n",
      "Iteration 100000 of 113228   ETR: 16.01645248697281\n",
      "After epoch 33 the loss is 107777.47247317061\n",
      "Iteration 20000 of 113228   ETR: 113.09257888741493\n",
      "Iteration 40000 of 113228   ETR: 88.50985351934432\n",
      "Iteration 60000 of 113228   ETR: 64.39701028259594\n",
      "Iteration 80000 of 113228   ETR: 40.161057516252995\n",
      "Iteration 100000 of 113228   ETR: 15.973214380359648\n",
      "After epoch 34 the loss is 106898.0680611264\n",
      "Iteration 20000 of 113228   ETR: 112.02695161185264\n",
      "Iteration 40000 of 113228   ETR: 88.14273573276996\n",
      "Iteration 60000 of 113228   ETR: 64.3187320452849\n",
      "Iteration 80000 of 113228   ETR: 40.0935899181962\n",
      "Iteration 100000 of 113228   ETR: 15.968306908369064\n",
      "After epoch 35 the loss is 106040.96464590542\n",
      "Iteration 20000 of 113228   ETR: 112.00824846096039\n",
      "Iteration 40000 of 113228   ETR: 88.91918609116077\n",
      "Iteration 60000 of 113228   ETR: 64.3471958782196\n",
      "Iteration 80000 of 113228   ETR: 40.11108187712431\n",
      "Iteration 100000 of 113228   ETR: 15.954645462579727\n",
      "After epoch 36 the loss is 105206.68517587148\n",
      "Iteration 20000 of 113228   ETR: 112.67661632165908\n",
      "Iteration 40000 of 113228   ETR: 88.4382645448923\n",
      "Iteration 60000 of 113228   ETR: 64.22800417267482\n",
      "Iteration 80000 of 113228   ETR: 40.539200339698795\n",
      "Iteration 100000 of 113228   ETR: 16.089665674085616\n",
      "After epoch 37 the loss is 104395.60897446424\n",
      "Iteration 20000 of 113228   ETR: 113.44312210121156\n",
      "Iteration 40000 of 113228   ETR: 88.71910735359192\n",
      "Iteration 60000 of 113228   ETR: 64.64695816353162\n",
      "Iteration 80000 of 113228   ETR: 40.27974552479982\n",
      "Iteration 100000 of 113228   ETR: 16.0226857853508\n",
      "After epoch 38 the loss is 103607.96782354824\n",
      "Iteration 20000 of 113228   ETR: 113.45247645506859\n",
      "Iteration 40000 of 113228   ETR: 88.98343533744811\n",
      "Iteration 60000 of 113228   ETR: 64.70121996909776\n",
      "Iteration 80000 of 113228   ETR: 40.424675234770774\n",
      "Iteration 100000 of 113228   ETR: 16.094175483312608\n",
      "After epoch 39 the loss is 102843.84867347218\n",
      "Iteration 20000 of 113228   ETR: 113.02708396267892\n",
      "Iteration 40000 of 113228   ETR: 88.70072704360486\n",
      "Iteration 60000 of 113228   ETR: 64.46548824720382\n",
      "Iteration 80000 of 113228   ETR: 40.35262119394541\n",
      "Iteration 100000 of 113228   ETR: 16.059291196231843\n",
      "After epoch 40 the loss is 102103.20256975293\n",
      "Iteration 20000 of 113228   ETR: 113.33095209589004\n",
      "Iteration 40000 of 113228   ETR: 88.67505632688999\n",
      "Iteration 60000 of 113228   ETR: 64.6380648417155\n",
      "Iteration 80000 of 113228   ETR: 40.349296456182\n",
      "Iteration 100000 of 113228   ETR: 16.050406623535157\n",
      "After epoch 41 the loss is 101385.8565355055\n",
      "Iteration 20000 of 113228   ETR: 112.29335009551049\n",
      "Iteration 40000 of 113228   ETR: 88.01791190547942\n",
      "Iteration 60000 of 113228   ETR: 64.16485031434695\n",
      "Iteration 80000 of 113228   ETR: 40.01654619601965\n",
      "Iteration 100000 of 113228   ETR: 15.947748793706893\n",
      "After epoch 42 the loss is 100691.52967267111\n",
      "Iteration 20000 of 113228   ETR: 111.92412595810889\n",
      "Iteration 40000 of 113228   ETR: 89.51575200331212\n",
      "Iteration 60000 of 113228   ETR: 64.7697091436863\n",
      "Iteration 80000 of 113228   ETR: 40.41009794214964\n",
      "Iteration 100000 of 113228   ETR: 16.061680137386322\n",
      "After epoch 43 the loss is 100019.84903091937\n",
      "Iteration 20000 of 113228   ETR: 113.33563093986511\n",
      "Iteration 40000 of 113228   ETR: 88.76316230854988\n",
      "Iteration 60000 of 113228   ETR: 64.49663507267634\n",
      "Iteration 80000 of 113228   ETR: 40.20270299094915\n",
      "Iteration 100000 of 113228   ETR: 16.005576698961256\n",
      "After epoch 44 the loss is 99370.36782584898\n",
      "Iteration 20000 of 113228   ETR: 116.83632310881615\n",
      "Iteration 40000 of 113228   ETR: 89.91957497780324\n",
      "Iteration 60000 of 113228   ETR: 64.98763666669528\n",
      "Iteration 80000 of 113228   ETR: 40.40385200228691\n",
      "Iteration 100000 of 113228   ETR: 16.068046620779036\n",
      "After epoch 45 the loss is 98742.58180872165\n",
      "Iteration 20000 of 113228   ETR: 144.31486926054956\n",
      "Iteration 40000 of 113228   ETR: 103.58700648965836\n",
      "Iteration 60000 of 113228   ETR: 71.54487275571823\n",
      "Iteration 80000 of 113228   ETR: 43.559280163872245\n",
      "Iteration 100000 of 113228   ETR: 17.063433102149965\n",
      "After epoch 46 the loss is 98135.94464580435\n",
      "Iteration 20000 of 113228   ETR: 114.5134348889351\n",
      "Iteration 40000 of 113228   ETR: 89.35054744987488\n",
      "Iteration 60000 of 113228   ETR: 64.7234554944833\n",
      "Iteration 80000 of 113228   ETR: 40.51046523349285\n",
      "Iteration 100000 of 113228   ETR: 16.070301809234618\n",
      "After epoch 47 the loss is 97549.88134746905\n",
      "Iteration 20000 of 113228   ETR: 112.2092242585659\n",
      "Iteration 40000 of 113228   ETR: 88.02525381600857\n",
      "Iteration 60000 of 113228   ETR: 64.13994237187704\n",
      "Iteration 80000 of 113228   ETR: 39.99697229014635\n",
      "Iteration 100000 of 113228   ETR: 15.93050623846054\n",
      "After epoch 48 the loss is 96983.7990374919\n",
      "Iteration 20000 of 113228   ETR: 112.12509953298569\n",
      "Iteration 40000 of 113228   ETR: 89.70848664798737\n",
      "Iteration 60000 of 113228   ETR: 64.99119509526889\n",
      "Iteration 80000 of 113228   ETR: 40.38552742302418\n",
      "Iteration 100000 of 113228   ETR: 16.038204283761978\n",
      "After epoch 49 the loss is 96437.09633179475\n",
      "Iteration 20000 of 113228   ETR: 112.48031381120681\n",
      "Iteration 40000 of 113228   ETR: 88.6823999833107\n",
      "Iteration 60000 of 113228   ETR: 64.34897678457897\n",
      "Iteration 80000 of 113228   ETR: 40.11774600867033\n",
      "Iteration 100000 of 113228   ETR: 15.949473386688233\n",
      "After epoch 50 the loss is 95909.16960942093\n",
      "Iteration 20000 of 113228   ETR: 113.55061103982925\n",
      "Iteration 40000 of 113228   ETR: 88.61447650821209\n",
      "Iteration 60000 of 113228   ETR: 64.2173307905356\n",
      "Iteration 80000 of 113228   ETR: 40.094839937996866\n",
      "Iteration 100000 of 113228   ETR: 15.959421452608108\n",
      "After epoch 51 the loss is 95399.41882494185\n",
      "Iteration 20000 of 113228   ETR: 112.21857194423677\n",
      "Iteration 40000 of 113228   ETR: 88.05829655988217\n",
      "Iteration 60000 of 113228   ETR: 64.07501183737119\n",
      "Iteration 80000 of 113228   ETR: 40.01196460558176\n",
      "Iteration 100000 of 113228   ETR: 15.947881442575454\n",
      "After epoch 52 the loss is 94907.2507233033\n",
      "Iteration 20000 of 113228   ETR: 112.05032582716942\n",
      "Iteration 40000 of 113228   ETR: 88.111530103302\n",
      "Iteration 60000 of 113228   ETR: 64.10614279966354\n",
      "Iteration 80000 of 113228   ETR: 40.131488205277925\n",
      "Iteration 100000 of 113228   ETR: 15.989528109683992\n",
      "After epoch 53 the loss is 94432.08205749188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20000 of 113228   ETR: 112.09706425552368\n",
      "Iteration 40000 of 113228   ETR: 88.27857089819908\n",
      "Iteration 60000 of 113228   ETR: 64.08123697228432\n",
      "Iteration 80000 of 113228   ETR: 40.09650666407347\n",
      "Iteration 100000 of 113228   ETR: 15.953849916286469\n",
      "After epoch 54 the loss is 93973.34164659306\n",
      "Iteration 20000 of 113228   ETR: 111.91945933914185\n",
      "Iteration 40000 of 113228   ETR: 88.04177911620141\n",
      "Iteration 60000 of 113228   ETR: 64.27425930244128\n",
      "Iteration 80000 of 113228   ETR: 40.08151434863806\n",
      "Iteration 100000 of 113228   ETR: 16.014198118505476\n",
      "After epoch 55 the loss is 93530.47112343926\n",
      "Iteration 20000 of 113228   ETR: 112.49901251664161\n",
      "Iteration 40000 of 113228   ETR: 88.56124820246697\n",
      "Iteration 60000 of 113228   ETR: 64.41657804409662\n",
      "Iteration 80000 of 113228   ETR: 40.131072093164924\n",
      "Iteration 100000 of 113228   ETR: 15.961012072124483\n",
      "After epoch 56 the loss is 93102.92625252903\n",
      "Iteration 20000 of 113228   ETR: 112.22793630037309\n",
      "Iteration 40000 of 113228   ETR: 88.56308400745392\n",
      "Iteration 60000 of 113228   ETR: 64.2564701207002\n",
      "Iteration 80000 of 113228   ETR: 40.059858396792414\n",
      "Iteration 100000 of 113228   ETR: 15.943902512664794\n",
      "After epoch 57 the loss is 92690.17751936521\n",
      "Iteration 20000 of 113228   ETR: 113.52258243055344\n",
      "Iteration 40000 of 113228   ETR: 88.44377457869052\n",
      "Iteration 60000 of 113228   ETR: 64.21467529417673\n",
      "Iteration 80000 of 113228   ETR: 40.08651175410748\n",
      "Iteration 100000 of 113228   ETR: 15.954645935649872\n",
      "After epoch 58 the loss is 92291.70988939237\n",
      "Iteration 20000 of 113228   ETR: 112.43358427376748\n",
      "Iteration 40000 of 113228   ETR: 89.07337930645942\n",
      "Iteration 60000 of 113228   ETR: 64.49752510283788\n",
      "Iteration 80000 of 113228   ETR: 40.14023497695923\n",
      "Iteration 100000 of 113228   ETR: 15.98833449063301\n",
      "After epoch 59 the loss is 91907.02369011566\n",
      "Iteration 20000 of 113228   ETR: 112.51302348718644\n",
      "Iteration 40000 of 113228   ETR: 88.57137699241638\n",
      "Iteration 60000 of 113228   ETR: 64.40369862236977\n",
      "Iteration 80000 of 113228   ETR: 40.1696019754529\n",
      "Iteration 100000 of 113228   ETR: 16.030580569152832\n",
      "After epoch 60 the loss is 91535.63374129962\n",
      "Iteration 20000 of 113228   ETR: 112.78878521561623\n",
      "Iteration 40000 of 113228   ETR: 88.42541521940232\n",
      "Iteration 60000 of 113228   ETR: 64.2271166806221\n",
      "Iteration 80000 of 113228   ETR: 40.091925766825675\n",
      "Iteration 100000 of 113228   ETR: 15.954911643381118\n",
      "After epoch 61 the loss is 91177.06987948809\n",
      "Iteration 20000 of 113228   ETR: 112.38683695449828\n",
      "Iteration 40000 of 113228   ETR: 88.30426693034173\n",
      "Iteration 60000 of 113228   ETR: 64.43614622861544\n",
      "Iteration 80000 of 113228   ETR: 40.24726352806092\n",
      "Iteration 100000 of 113228   ETR: 15.99602692656517\n",
      "After epoch 62 the loss is 90830.87634224072\n",
      "Iteration 20000 of 113228   ETR: 112.04564698319435\n",
      "Iteration 40000 of 113228   ETR: 88.00506738119127\n",
      "Iteration 60000 of 113228   ETR: 64.16307110006014\n",
      "Iteration 80000 of 113228   ETR: 40.032371329844\n",
      "Iteration 100000 of 113228   ETR: 15.929180285921095\n",
      "After epoch 63 the loss is 90496.6116062291\n",
      "Iteration 20000 of 113228   ETR: 112.31205213503839\n",
      "Iteration 40000 of 113228   ETR: 88.46763044111728\n",
      "Iteration 60000 of 113228   ETR: 64.28582102266948\n",
      "Iteration 80000 of 113228   ETR: 40.290990751647946\n",
      "Iteration 100000 of 113228   ETR: 16.034225322313308\n",
      "After epoch 64 the loss is 90173.84802074265\n",
      "Iteration 20000 of 113228   ETR: 113.30758677148819\n",
      "Iteration 40000 of 113228   ETR: 88.48966359274388\n",
      "Iteration 60000 of 113228   ETR: 64.14973080007235\n",
      "Iteration 80000 of 113228   ETR: 40.009466744577885\n",
      "Iteration 100000 of 113228   ETR: 15.99138525691986\n",
      "After epoch 65 the loss is 89862.1717609195\n",
      "Iteration 20000 of 113228   ETR: 112.81682938399315\n",
      "Iteration 40000 of 113228   ETR: 88.65119479031563\n",
      "Iteration 60000 of 113228   ETR: 64.40056828815142\n",
      "Iteration 80000 of 113228   ETR: 40.0877604865551\n",
      "Iteration 100000 of 113228   ETR: 15.98236595384598\n",
      "After epoch 66 the loss is 89561.18231945299\n",
      "Iteration 20000 of 113228   ETR: 112.6766063193798\n",
      "Iteration 40000 of 113228   ETR: 88.34464634706974\n",
      "Iteration 60000 of 113228   ETR: 64.14172200918198\n",
      "Iteration 80000 of 113228   ETR: 40.0182114366889\n",
      "Iteration 100000 of 113228   ETR: 15.970561466064455\n",
      "After epoch 67 the loss is 89270.49208124634\n",
      "Iteration 20000 of 113228   ETR: 113.03657168025971\n",
      "Iteration 40000 of 113228   ETR: 88.73195667908192\n",
      "Iteration 60000 of 113228   ETR: 64.50374939171473\n",
      "Iteration 80000 of 113228   ETR: 40.29973791943788\n",
      "Iteration 100000 of 113228   ETR: 16.021890743665693\n",
      "After epoch 68 the loss is 88989.72599342745\n",
      "Iteration 20000 of 113228   ETR: 113.22812533078194\n",
      "Iteration 40000 of 113228   ETR: 88.7705059649706\n",
      "Iteration 60000 of 113228   ETR: 64.90224474487304\n",
      "Iteration 80000 of 113228   ETR: 40.398021085238454\n",
      "Iteration 100000 of 113228   ETR: 16.05239594656944\n",
      "After epoch 69 the loss is 88718.52149462234\n",
      "Iteration 20000 of 113228   ETR: 111.92412262401581\n",
      "Iteration 40000 of 113228   ETR: 88.01608221111297\n",
      "Iteration 60000 of 113228   ETR: 64.27425803338687\n",
      "Iteration 80000 of 113228   ETR: 40.128989254975316\n",
      "Iteration 100000 of 113228   ETR: 15.974010305109024\n",
      "After epoch 70 the loss is 88456.52788503002\n",
      "Iteration 20000 of 113228   ETR: 112.82618262648582\n",
      "Iteration 40000 of 113228   ETR: 90.22244831929206\n",
      "Iteration 60000 of 113228   ETR: 65.07302837700844\n",
      "Iteration 80000 of 113228   ETR: 40.43925034879446\n",
      "Iteration 100000 of 113228   ETR: 16.06313940109253\n",
      "After epoch 71 the loss is 88203.40619453788\n",
      "Iteration 20000 of 113228   ETR: 112.73736905465127\n",
      "Iteration 40000 of 113228   ETR: 88.54472813994884\n",
      "Iteration 60000 of 113228   ETR: 64.31428485560417\n",
      "Iteration 80000 of 113228   ETR: 40.23768502728939\n",
      "Iteration 100000 of 113228   ETR: 15.982631030817032\n",
      "After epoch 72 the loss is 87958.82878647558\n",
      "Iteration 20000 of 113228   ETR: 112.76540766620637\n",
      "Iteration 40000 of 113228   ETR: 88.23818668026924\n",
      "Iteration 60000 of 113228   ETR: 64.4192470770359\n",
      "Iteration 80000 of 113228   ETR: 40.1273230240345\n",
      "Iteration 100000 of 113228   ETR: 15.98130463674545\n",
      "After epoch 73 the loss is 87722.47902160976\n",
      "Iteration 20000 of 113228   ETR: 111.94755018482208\n",
      "Iteration 40000 of 113228   ETR: 88.13723268253803\n",
      "Iteration 60000 of 113228   ETR: 64.10258585165342\n",
      "Iteration 80000 of 113228   ETR: 40.01154879055023\n",
      "Iteration 100000 of 113228   ETR: 15.949605940942764\n",
      "After epoch 74 the loss is 87494.05112571921\n",
      "Iteration 20000 of 113228   ETR: 112.34009074659348\n",
      "Iteration 40000 of 113228   ETR: 88.28774424898624\n",
      "Iteration 60000 of 113228   ETR: 64.06611724650065\n",
      "Iteration 80000 of 113228   ETR: 40.392606478357315\n",
      "Iteration 100000 of 113228   ETR: 16.057701112861633\n",
      "After epoch 75 the loss is 87273.24967319984\n",
      "Iteration 20000 of 113228   ETR: 112.27467250599861\n",
      "Iteration 40000 of 113228   ETR: 88.18311907775401\n",
      "Iteration 60000 of 113228   ETR: 64.25558368619284\n",
      "Iteration 80000 of 113228   ETR: 40.15522640115022\n",
      "Iteration 100000 of 113228   ETR: 15.96790921406746\n",
      "After epoch 76 the loss is 87059.78946409747\n",
      "Iteration 20000 of 113228   ETR: 112.23728843150138\n",
      "Iteration 40000 of 113228   ETR: 88.09133974022865\n",
      "Iteration 60000 of 113228   ETR: 64.26091667585374\n",
      "Iteration 80000 of 113228   ETR: 40.078598394978044\n",
      "Iteration 100000 of 113228   ETR: 15.965521660585402\n",
      "After epoch 77 the loss is 86853.39556311816\n",
      "Iteration 20000 of 113228   ETR: 112.3541295012474\n",
      "Iteration 40000 of 113228   ETR: 88.86779184451103\n",
      "Iteration 60000 of 113228   ETR: 64.42369490124385\n",
      "Iteration 80000 of 113228   ETR: 40.21561286430359\n",
      "Iteration 100000 of 113228   ETR: 16.09880447467804\n",
      "After epoch 78 the loss is 86653.80223629065\n",
      "Iteration 20000 of 113228   ETR: 113.30758454875946\n",
      "Iteration 40000 of 113228   ETR: 88.91184767241478\n",
      "Iteration 60000 of 113228   ETR: 64.38366871355375\n",
      "Iteration 80000 of 113228   ETR: 40.173967092525956\n",
      "Iteration 100000 of 113228   ETR: 15.97069452492714\n",
      "After epoch 79 the loss is 86460.75352864945\n",
      "Iteration 20000 of 113228   ETR: 114.37788733420372\n",
      "Iteration 40000 of 113228   ETR: 88.89164988930226\n",
      "Iteration 60000 of 113228   ETR: 64.36944367113114\n",
      "Iteration 80000 of 113228   ETR: 40.07776478437185\n",
      "Iteration 100000 of 113228   ETR: 15.950666564207076\n",
      "After epoch 80 the loss is 86274.00241153967\n",
      "Iteration 20000 of 113228   ETR: 111.73251118254662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40000 of 113228   ETR: 87.96285303242206\n",
      "Iteration 60000 of 113228   ETR: 63.9531519449234\n",
      "Iteration 80000 of 113228   ETR: 39.958658683168885\n",
      "Iteration 100000 of 113228   ETR: 15.949207805109024\n",
      "After epoch 81 the loss is 86093.31120876409\n",
      "Iteration 20000 of 113228   ETR: 112.33536189122199\n",
      "Iteration 40000 of 113228   ETR: 88.3097494663477\n",
      "Iteration 60000 of 113228   ETR: 64.10079436984061\n",
      "Iteration 80000 of 113228   ETR: 40.17479377123118\n",
      "Iteration 100000 of 113228   ETR: 15.976660790977476\n",
      "After epoch 82 the loss is 85918.45045044366\n",
      "Iteration 20000 of 113228   ETR: 112.67194859132766\n",
      "Iteration 40000 of 113228   ETR: 88.73195624260902\n",
      "Iteration 60000 of 113228   ETR: 64.46994960799218\n",
      "Iteration 80000 of 113228   ETR: 40.2335202421546\n",
      "Iteration 100000 of 113228   ETR: 16.068842640142442\n",
      "After epoch 83 the loss is 85749.19950708095\n",
      "Iteration 20000 of 113228   ETR: 112.66259423747061\n",
      "Iteration 40000 of 113228   ETR: 88.57592940468788\n",
      "Iteration 60000 of 113228   ETR: 64.67186314487458\n",
      "Iteration 80000 of 113228   ETR: 40.29890440785885\n",
      "Iteration 100000 of 113228   ETR: 16.02878725484848\n",
      "After epoch 84 the loss is 85585.34578117821\n",
      "Iteration 20000 of 113228   ETR: 112.17183129315377\n",
      "Iteration 40000 of 113228   ETR: 88.58511061198712\n",
      "Iteration 60000 of 113228   ETR: 64.3267387210846\n",
      "Iteration 80000 of 113228   ETR: 40.17271766688824\n",
      "Iteration 100000 of 113228   ETR: 16.000669069280626\n",
      "After epoch 85 the loss is 85426.68441537162\n",
      "Iteration 20000 of 113228   ETR: 112.33541968216898\n",
      "Iteration 40000 of 113228   ETR: 88.34097953829766\n",
      "Iteration 60000 of 113228   ETR: 64.13994385244051\n",
      "Iteration 80000 of 113228   ETR: 40.046114714777474\n",
      "Iteration 100000 of 113228   ETR: 15.955043850717546\n",
      "After epoch 86 the loss is 85273.01895563724\n",
      "Iteration 20000 of 113228   ETR: 113.10197547311783\n",
      "Iteration 40000 of 113228   ETR: 88.66954672956467\n",
      "Iteration 60000 of 113228   ETR: 64.34808717743556\n",
      "Iteration 80000 of 113228   ETR: 40.12690661484003\n",
      "Iteration 100000 of 113228   ETR: 15.968174259500502\n",
      "After epoch 87 the loss is 85124.15971233882\n",
      "Iteration 20000 of 113228   ETR: 112.51302237582206\n",
      "Iteration 40000 of 113228   ETR: 88.24185392551423\n",
      "Iteration 60000 of 113228   ETR: 64.20220852664312\n",
      "Iteration 80000 of 113228   ETR: 40.06152373648882\n",
      "Iteration 100000 of 113228   ETR: 16.02348189932823\n",
      "After epoch 88 the loss is 84979.92479122849\n",
      "Iteration 20000 of 113228   ETR: 114.3965838169098\n",
      "Iteration 40000 of 113228   ETR: 89.04033438022137\n",
      "Iteration 60000 of 113228   ETR: 64.51620241115887\n",
      "Iteration 80000 of 113228   ETR: 40.273501070344444\n",
      "Iteration 100000 of 113228   ETR: 16.015922049188614\n",
      "After epoch 89 the loss is 84840.13904572511\n",
      "Iteration 20000 of 113228   ETR: 112.57846173233985\n",
      "Iteration 40000 of 113228   ETR: 88.24186003613471\n",
      "Iteration 60000 of 113228   ETR: 64.10258712070784\n",
      "Iteration 80000 of 113228   ETR: 40.1348205681324\n",
      "Iteration 100000 of 113228   ETR: 15.954910949544907\n",
      "After epoch 90 the loss is 84704.63435053639\n",
      "Iteration 20000 of 113228   ETR: 112.11574851322175\n",
      "Iteration 40000 of 113228   ETR: 87.91145092926025\n",
      "Iteration 60000 of 113228   ETR: 64.1008060028394\n",
      "Iteration 80000 of 113228   ETR: 39.939501483571526\n",
      "Iteration 100000 of 113228   ETR: 15.909152577505111\n",
      "After epoch 91 the loss is 84573.2491783388\n",
      "Iteration 20000 of 113228   ETR: 112.30270778346062\n",
      "Iteration 40000 of 113228   ETR: 88.45845621738434\n",
      "Iteration 60000 of 113228   ETR: 64.09369274134636\n",
      "Iteration 80000 of 113228   ETR: 40.00155417766571\n",
      "Iteration 100000 of 113228   ETR: 15.923477046871184\n",
      "After epoch 92 the loss is 84445.82833600044\n",
      "Iteration 20000 of 113228   ETR: 114.12083542485237\n",
      "Iteration 40000 of 113228   ETR: 89.26244277763367\n",
      "Iteration 60000 of 113228   ETR: 64.57757980481783\n",
      "Iteration 80000 of 113228   ETR: 40.23893534417152\n",
      "Iteration 100000 of 113228   ETR: 15.990854661445617\n",
      "After epoch 93 the loss is 84322.22260813508\n",
      "Iteration 20000 of 113228   ETR: 112.9056151717186\n",
      "Iteration 40000 of 113228   ETR: 88.27489448702336\n",
      "Iteration 60000 of 113228   ETR: 64.07234111235937\n",
      "Iteration 80000 of 113228   ETR: 40.00904825581312\n",
      "Iteration 100000 of 113228   ETR: 15.932363133392334\n",
      "After epoch 94 the loss is 84202.28922193451\n",
      "Iteration 20000 of 113228   ETR: 112.31205991458891\n",
      "Iteration 40000 of 113228   ETR: 88.67505370805263\n",
      "Iteration 60000 of 113228   ETR: 64.36587657068571\n",
      "Iteration 80000 of 113228   ETR: 40.15439358276129\n",
      "Iteration 100000 of 113228   ETR: 16.02242118144989\n",
      "After epoch 95 the loss is 84085.89065265888\n",
      "Iteration 20000 of 113228   ETR: 112.2886845879078\n",
      "Iteration 40000 of 113228   ETR: 88.33731054716111\n",
      "Iteration 60000 of 113228   ETR: 64.16929961911838\n",
      "Iteration 80000 of 113228   ETR: 40.09484082924128\n",
      "Iteration 100000 of 113228   ETR: 15.97188801782608\n",
      "After epoch 96 the loss is 83972.89514672244\n",
      "Iteration 20000 of 113228   ETR: 111.75587206149102\n",
      "Iteration 40000 of 113228   ETR: 88.01607784638404\n",
      "Iteration 60000 of 113228   ETR: 64.1879767159303\n",
      "Iteration 80000 of 113228   ETR: 40.460905805504325\n",
      "Iteration 100000 of 113228   ETR: 16.060619167203903\n",
      "After epoch 97 the loss is 83863.17626928166\n",
      "Iteration 20000 of 113228   ETR: 112.21857861242295\n",
      "Iteration 40000 of 113228   ETR: 87.8710732584238\n",
      "Iteration 60000 of 113228   ETR: 65.16109017780622\n",
      "Iteration 80000 of 113228   ETR: 43.24840836957693\n",
      "Iteration 100000 of 113228   ETR: 17.277293417835235\n",
      "After epoch 98 the loss is 83756.61290295701\n",
      "Iteration 20000 of 113228   ETR: 111.86804428954125\n",
      "Iteration 40000 of 113228   ETR: 88.22350504157544\n",
      "Iteration 60000 of 113228   ETR: 63.966494783020025\n",
      "Iteration 80000 of 113228   ETR: 40.06735495061874\n",
      "Iteration 100000 of 113228   ETR: 15.974540869045258\n",
      "After epoch 99 the loss is 83653.08835630165\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "window_size = 2\n",
    "num_epochs = 100\n",
    "batch_size = 50\n",
    "most_common = 18000 # Take only the N most common words into account\n",
    "\n",
    "training_en = read_sentences('training.en')\n",
    "word2idx, idx2word, vocabulary_size = create_vocabulary(training_en,most_common)\n",
    "print ('The vocabulary size is',vocabulary_size)\n",
    "pairs = create_center_context_pairs(training_en,window_size,word2idx)\n",
    "print ('There are',pairs.shape[0],'pairs')\n",
    "print ('Starting to train')\n",
    "w2v = W2V(pairs,embedding_dim,vocabulary_size,window_size,num_epochs)\n",
    "w2v.train_batch(idx2word,word2idx,batch_size,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to get the candidates and the test sentences\n",
    "\n",
    "def get_candidates(fileName='lst/lst.gold.candidates'):\n",
    "    candidates = {}\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    with open(fileName, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            target, options = line.split('::')\n",
    "            candidates[target] = [x.strip() for x in options.split(';')]\n",
    "            \n",
    "    return candidates\n",
    "\n",
    "def get_test_sentences(fileName='lst/lst_test.preprocessed'):\n",
    "    \n",
    "    with open (fileName,'r') as testFile:\n",
    "        lines = testFile.readlines()\n",
    "    \n",
    "    return [line.split() for line in lines]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_word(word,word2embs):\n",
    "    # Find the closest word of a given word\n",
    "    # INPUT\n",
    "    # - word, a string, like 'money'\n",
    "    # - word2embs, dictionary mapping words (string) to numpy arrays\n",
    "    # OUTPUT\n",
    "    # - closest_words, a sorted list of tuples of (word,score). The first elements in the list have the highest scores\n",
    "    \n",
    "    # Check if word actually exists\n",
    "    assert word in list(word2embs.keys())\n",
    "    \n",
    "    # Retrieve the word embedding of the target word\n",
    "    word_embedding = word2embs[word]\n",
    "    closest_words = []\n",
    "    \n",
    "    for w, e in word2embs.items():\n",
    "        distance =  1 - sd.cosine(word_embedding,e)\n",
    "\n",
    "        closest_words.append((w,distance))\n",
    "        \n",
    "    # Sort the list and reverse for high to low\n",
    "    return sorted(closest_words,key=operator.itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('money', 1.0), ('dribs', 0.27765050530433655), ('87.5', 0.25810831785202026), ('rence', 0.2482018619775772), ('laundering', 0.24645136296749115), ('bec', 0.23659881949424744), ('funds', 0.21699900925159454), ('effecting', 0.2161044031381607), ('incurred', 0.2126823216676712), ('immunity', 0.20829153060913086)]\n",
      "('couronne', -0.22050049901008606)\n"
     ]
    }
   ],
   "source": [
    "closest = find_closest_word('money',word2embs_W2)\n",
    "print (closest[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embs_W1 = pickle.load(open('SG/word2embed_W1.pickle','rb'))\n",
    "word2embs_W2 = pickle.load(open('SG/word2embed_W2.pickle','rb'))\n",
    "word2idx = pickle.load(open('SG/word2idx.pickle','rb')) \n",
    "word2idx = defaultdict(lambda: word2idx[\"<unk>\"], word2idx)\n",
    "word2embs_W1 = defaultdict(lambda: word2embs_W1[\"<unk>\"], word2embs_W1)\n",
    "word2embs_W2 = defaultdict(lambda: word2embs_W2[\"<unk>\"], word2embs_W2)\n",
    "\n",
    "idx2word = pickle.load(open('SG/idx2word.pickle','rb'))\n",
    "word2embs = pickle.load(open('skipgram.pickle','rb'))\n",
    "\n",
    "run_lst(word2embs_W1,word2embs_W2, word2idx, evaluate_skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_skipgram(target_emb,context_emb, sentence, target, candidates, target_idx, w2i=None):\n",
    "    '''target_emb:  the W1 matrix of the skip gram model\n",
    "       context_emb: the W2 matrix of the skip gram model\n",
    "       sentence: the sentence of the lexical subsitution task. a string\n",
    "       target: a word\n",
    "       candidats: list of words\n",
    "       target_idx: index of the target word\n",
    "       w2i: not needed for skipgram (but n_args should match bsg)'''\n",
    "\n",
    "    # Split the word, because it is still in the format: 'side.n'\n",
    "    target = target.split('.')[0]\n",
    "        \n",
    "    # If we can not retrieve the target word embedding, return an empty list\n",
    "    '''\n",
    "    try:\n",
    "        vector = embeddings[target]\n",
    "    except:\n",
    "        #print (target,'not in dict')\n",
    "        return []\n",
    "    '''\n",
    "\n",
    "    scores = []\n",
    "    \n",
    "    # Retrieve the context words\n",
    "    window_size = 1\n",
    "    context_words = []\n",
    "    for i in range(target_idx-window_size,target_idx+window_size+1):\n",
    "        try:\n",
    "            if i != target_idx:\n",
    "                context_words.append(sentence[i])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    # Loop over the words in the candidate list\n",
    "    for word in candidates:\n",
    "        successes = 0\n",
    "        \n",
    "        try:  \n",
    "            vector = target_emb[target]\n",
    "            score = 1 - sc.spatial.distance.cosine(vector,target_emb[word])\n",
    "            successes += 1\n",
    "        except:\n",
    "            score = np.zeros([300,])\n",
    "            \n",
    "        for c_word in context_words:\n",
    "            try:\n",
    "                new_score = 1 - sc.spatial.distance.cosine(target_emb[c_word],context_emb[word])\n",
    "                score += new_score\n",
    "                successes += 1\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        score = score / successes \n",
    "        scores.append((word,score))    \n",
    "        \n",
    "    return scores\n",
    "        \n",
    "def run_lst(W1,W2, w2i, eval_func):\n",
    "    \n",
    "    # Load evaluation data\n",
    "    sentences = get_test_sentences('lst/lst_test.preprocessed')\n",
    "    all_candidates = get_candidates('lst/lst.gold.candidates')\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Encode the target withouth the POS-tag\n",
    "        target = sentence[0]\n",
    "        target_stripped = target[:-2]\n",
    "        \n",
    "        # Extract id's\n",
    "        sent_id = sentence[1]\n",
    "        targ_idx = int(sentence[2])\n",
    "        \n",
    "        # Remove target word from sentence to obtain context\n",
    "        context = sentence.copy()\n",
    "        del(context[targ_idx])\n",
    "        \n",
    "        scores = eval_func(W1,W2 , sentence[3:], target_stripped, all_candidates[target],targ_idx, w2i)\n",
    "\n",
    "        result.append(format_ranking_string(target, sent_id, scores))\n",
    "    \n",
    "    # save output to file\n",
    "    with open(os.path.join('lst', 'lst.out'), 'w') as f:\n",
    "        f.writelines(result)\n",
    "        \n",
    "    # call evaluation scripts\n",
    "    subprocess.Popen('python lst_gap.py lst/lst_test.gold lst/lst.out lst/out no-mwe'.split())\n",
    "        \n",
    "def format_ranking_string(target, sentence_id, scores):\n",
    "        \n",
    "    base = \"RANKED\\t{} {}\".format(target, sentence_id)\n",
    "    candidates = \"\".join(\"\\t{} {}\".format(x[0], x[1]) for x in scores)\n",
    "    \n",
    "    return base + candidates + \"\\n\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
