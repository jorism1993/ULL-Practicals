{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oim7Ho4SJBWL"
   },
   "source": [
    "# Learning Word Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "c1B5E-8_JBWN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google not needed on local runtime\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import io\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import os\n",
    "import subprocess\n",
    "import itertools\n",
    "import sys\n",
    "import cProfile\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except:\n",
    "    print(\"Google not needed on local runtime\")\n",
    "    \n",
    "\n",
    "import time\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1NV_0SaiJwJu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.kl import kl_divergence\n",
    "import torch.optim as optim\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_gbaioJzJBWV"
   },
   "outputs": [],
   "source": [
    "def read_sentences(file):\n",
    "    '''Read text in file and tokenize sentences'''\n",
    "    sentences = []\n",
    "\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    with open(file,encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            tokens = line.split()\n",
    "            tokens = [token.lower() for token in tokens if token not in stopWords]\n",
    "            tokens = list(filter(lambda x: x not in string.punctuation, tokens))\n",
    "            sentences.append(tokens)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def generate_emba_data(lang_1, lang_2, w2i_1, w2i_2):\n",
    "    \n",
    "    for i in range(len(lang_1)):\n",
    "        sentence1 = lang_1[i]\n",
    "        sentence2 = lang_2[i]\n",
    "        \n",
    "        # Skip if any of the sentences has length 0\n",
    "        if min(len(sentence1), len(sentence2)) == 0 or len(sentence1) == 1:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        sentence1_tensor = torch.LongTensor([w2i_1[x] for x in sentence1])\n",
    "        sentence2_tensor = torch.LongTensor([w2i_2[x] for x in sentence2])\n",
    "        \n",
    "        yield sentence1_tensor, sentence2_tensor\n",
    "\n",
    "def create_vocabulary(corpus, n=10000):\n",
    "    \n",
    "    all_words = list(itertools.chain(*corpus))\n",
    "    top_n = set([i[0] for i in Counter(all_words).most_common(n)])\n",
    "    vocabulary = set(all_words).intersection(top_n)\n",
    " \n",
    "\n",
    "    # Placeholder for unknown words, start en end of senctence\n",
    "    vocabulary.add(\"<unk>\")\n",
    "    vocabulary.add(\"<s>\")\n",
    "    vocabulary.add(\"</s>\")\n",
    "\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "\n",
    "    for (idx, word) in enumerate(list(vocabulary)):\n",
    "        word2idx[word] = idx\n",
    "        idx2word[idx] = word\n",
    "\n",
    "    # Return the ID for <unk> for new words\n",
    "    word2idx = defaultdict(lambda: word2idx[\"<unk>\"], word2idx)\n",
    "\n",
    "    vocabulary_size = len(vocabulary)\n",
    "\n",
    "    return word2idx, idx2word, vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "umcJStM7JBWY"
   },
   "outputs": [],
   "source": [
    "dev_en = read_sentences('wa/dev.en')\n",
    "dev_fr = read_sentences('wa/dev.fr')\n",
    "test_en = read_sentences('wa/test.en')\n",
    "test_fr = read_sentences('wa/test.fr')\n",
    "training_en = read_sentences('hansards/training.en')\n",
    "training_fr = read_sentences('hansards/training.fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q24a417CcYsw"
   },
   "source": [
    "# Embed Align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cDCCgj35ccAH"
   },
   "outputs": [],
   "source": [
    "class EmbedAlign(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_1, vocab_2, embeddings_dim):\n",
    "        super(EmbedAlign, self).__init__()\n",
    "        \n",
    "        # Encoder paramters\n",
    "        self.embed = torch.nn.Embedding(vocab_1, embeddings_dim)\n",
    "        self.linear_mu_1 = torch.nn.Linear(2*embeddings_dim, embeddings_dim)\n",
    "        self.linear_mu_2 = torch.nn.Linear(embeddings_dim, embeddings_dim)\n",
    "        self.linear_sig_1 = torch.nn.Linear(2*embeddings_dim, embeddings_dim)\n",
    "        \n",
    "        # Decode l1 language\n",
    "        self.linear_l1_1 = torch.nn.Linear(embeddings_dim, vocab_1)\n",
    "        \n",
    "        # Decode l2 language\n",
    "        self.linear_l2_1 = torch.nn.Linear(embeddings_dim, vocab_2)\n",
    "        \n",
    "    def reparameterize(self, mu, std):\n",
    "        # Sample from standard normal\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        # Multiply by scale, add location\n",
    "        z = eps.mul(std.data).add(mu.data)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        m = sentence.size(0)\n",
    "        \n",
    "        h_mu = self.embed(sentence)\n",
    "        \n",
    "        # Encode context by averaging over neighbours\n",
    "        context = []\n",
    "        for i in range(m):\n",
    "            other_idx = list(range(m))\n",
    "            other_idx.remove(i)\n",
    "            idx = torch.LongTensor(other_idx)\n",
    "            others = torch.index_select(h_mu, 0, idx)\n",
    "            avg = torch.mean(others, 0)\n",
    "            context.append(avg)\n",
    "            \n",
    "        # Concatenate target and context\n",
    "        context = torch.stack(context, 0)\n",
    "        h_mu = torch.cat((h_mu, context), 1)\n",
    "        \n",
    "        # Create a copy for mu and sigma MLPs\n",
    "        h_sig = torch.empty_like(h_mu)\n",
    "        h_sig.copy_(h_mu)\n",
    "        \n",
    "        # calculate mu\n",
    "        mu = self.linear_mu_1(h_mu)\n",
    "        mu = F.relu(mu)\n",
    "                \n",
    "        # calculate sigma\n",
    "        sig = self.linear_sig_1(h_sig)\n",
    "        sig = F.softplus(sig)\n",
    "        \n",
    "        return mu, sig\n",
    "    \n",
    "    def decode(self, l1_z):\n",
    "        l2_z = torch.empty_like(l1_z)\n",
    "        l2_z.copy_(l1_z)\n",
    "        \n",
    "        # l1 lang\n",
    "        l1_probs = self.linear_l1_1(l1_z)\n",
    "        l1_probs = F.relu(l1_probs)\n",
    "        l1_probs = F.log_softmax(l1_probs, dim=1)\n",
    "        \n",
    "        # l2 lang\n",
    "        l2_probs = self.linear_l2_1(l2_z)\n",
    "        l2_probs = F.relu(l2_probs)\n",
    "        l2_probs = F.log_softmax(l2_probs, dim=1)\n",
    "        \n",
    "        return l1_probs, l2_probs\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        mu, sigma = self.encode(sentence)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        l1_out, l2_out = self.decode(z)\n",
    "        return l1_out, l2_out, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TO9gpzq4x3-8"
   },
   "outputs": [],
   "source": [
    "def embalign_loss(l1_sent, l2_sent, l1_probs, l2_probs, mu, sigma):\n",
    "    \n",
    "    m = l1_sent.size(0)\n",
    "    n = l2_sent.size(0)\n",
    "    \n",
    "    l1_loss = F.nll_loss(l1_probs, l1_sent)\n",
    "    \n",
    "    l2_loss = 0\n",
    "    for j in range(n):\n",
    "        l2_loss += F.nll_loss(l2_probs, l2_sent[j].repeat(m)) / m\n",
    "\n",
    "    # Add epsilon to prevent nan values in log\n",
    "    sigma = sigma + 1e-6\n",
    "    kl_loss = torch.sum(-torch.log(sigma) + (sigma.pow(2) + mu.pow(2)) / 2 - 0.5, 1)\n",
    "        \n",
    "    return l1_loss + l2_loss + torch.sum(kl_loss, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gPKZ5HbOhCKb"
   },
   "outputs": [],
   "source": [
    "def init_model_dir(model_name, lr, embedding_dim):\n",
    "    label = \"{}_lr-{}_emb-{}\".format(model_name, lr, embedding_dim)\n",
    "    \n",
    "    # Create directory to save model and save each epoch\n",
    "    if not os.path.exists(label):\n",
    "        os.makedirs(label)\n",
    "        print(\"First time running hyperparamter settings, created directory.\")\n",
    "            \n",
    "    return label\n",
    "\n",
    "def train(n_epochs, embedding_dim, l1_corpus, l2_corpus, vocab_size, lr, label):\n",
    "    l1_w2i, _, l1_vocab = create_vocabulary(l1_corpus, n=vocab_size)\n",
    "    l2_w2i, _, l2_vocab = create_vocabulary(l2_corpus, n=vocab_size)\n",
    "    model = EmbedAlign(l1_vocab, l2_vocab, embedding_dim)\n",
    "    optimizer = Adam(model.parameters(), lr)\n",
    "        \n",
    "    # Create a directory to save model data (epochs and w2i mapping)\n",
    "    model_dir = init_model_dir(label, lr, embedding_dim)\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(model_dir, 'l1_w2i.pickle'), 'rb') as f:\n",
    "            l1_w2i = pickle.load(f)\n",
    "            l1_w2i = defaultdict(lambda: l1_w2i['<unk>'], l1_w2i)\n",
    "        \n",
    "        with open(os.path.join(model_dir, 'l2_w2i.pickle'), 'rb') as f:\n",
    "            l2_w2i = pickle.load(f)\n",
    "            l2_w2i = defaultdict(lambda: l1_w2i['<unk>'], l1_w2i)\n",
    "            \n",
    "        epoch_file_name = sorted(os.listdir(model_dir))[-3]\n",
    "        epoch_num = epoch_file_name.split('.')[0][3:]\n",
    "        \n",
    "        with open(os.path.join(model_dir, epoch_file_name), 'rb') as f:       \n",
    "            model = pickle.load(f)\n",
    "            \n",
    "        print(\"Continuing from epoch {}\".format(epoch_num))\n",
    "    except Exception as e:\n",
    "        print(\"No trained model found, starting from epoch 0\")\n",
    "        print(e)\n",
    "        epoch_num = 0\n",
    "    \n",
    "    # Save corresponding w2i dicts. The ordering can be different every time\n",
    "    # because vocabulary is created using sets. This way the model and ordering\n",
    "    # are always paired.\n",
    "    with open(os.path.join(model_dir, 'l1_w2i.pickle'), 'wb') as f:\n",
    "        pickle.dump(dict(l1_w2i), f)\n",
    "    with open(os.path.join(model_dir, 'l2_w2i.pickle'), 'wb') as f:\n",
    "        pickle.dump(dict(l2_w2i), f)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch < int(epoch_num): continue\n",
    "            \n",
    "        epoch_loss = 0\n",
    "        examples = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        data = generate_emba_data(l1_corpus, l2_corpus, l1_w2i, l2_w2i)\n",
    "        \n",
    "        for s1, s2 in data:\n",
    "  \n",
    "            examples += 1\n",
    "            \n",
    "            # Forward pass\n",
    "            l1_probs, l2_probs, mu, sigma = model(s1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = embalign_loss(s1, s2, l1_probs, l2_probs, mu, sigma)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                        \n",
    "            if examples % 1000 == 0:\n",
    "                sys.stdout.write(\"\\r{:.2f} target number {}\".format(time.time() - start, examples))\n",
    "                print()\n",
    "                            \n",
    "        with open(os.path.join(model_dir, 'ep-{}.pickle'.format(epoch)), 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        print(\"{:.2f} sec: EPOCH {}/{} \\t\\t LOSS\".format(time.time() - start, epoch, n_epochs),\n",
    "              epoch_loss.item() / examples)\n",
    "        \n",
    "    \n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6893,
     "status": "ok",
     "timestamp": 1526477102798,
     "user": {
      "displayName": "Aron Hammond",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "106989373078986092985"
     },
     "user_tz": -120
    },
    "id": "Eg1nq418F0Bh",
    "outputId": "57efb470-b02b-4a89-876c-f67a70ca0a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing from epoch 0\n",
      "22.82 target number 1000\n",
      "44.23 target number 2000\n",
      "64.81 target number 3000\n",
      "86.27 target number 4000\n",
      "107.05 target number 5000\n",
      "127.49 target number 6000\n",
      "148.43 target number 7000\n",
      "168.77 target number 8000\n",
      "189.99 target number 9000\n",
      "212.22 target number 10000\n",
      "233.75 target number 11000\n",
      "255.55 target number 12000\n",
      "278.34 target number 13000\n",
      "301.80 target number 14000\n",
      "324.79 target number 15000\n",
      "348.53 target number 16000\n",
      "369.33 target number 17000\n",
      "391.50 target number 18000\n",
      "412.90 target number 19000\n",
      "434.66 target number 20000\n",
      "458.94 target number 21000\n",
      "480.61 target number 22000\n",
      "504.03 target number 23000\n",
      "528.08 target number 24000\n",
      "552.18 target number 25000\n",
      "578.21 target number 26000\n",
      "603.00 target number 27000\n",
      "629.13 target number 28000\n",
      "655.20 target number 29000\n",
      "682.24 target number 30000\n",
      "712.17 target number 31000\n",
      "740.95 target number 32000\n",
      "770.48 target number 33000\n",
      "799.86 target number 34000\n",
      "830.75 target number 35000\n",
      "861.32 target number 36000\n",
      "892.76 target number 37000\n",
      "925.76 target number 38000\n",
      "958.51 target number 39000\n",
      "992.51 target number 40000\n",
      "1025.96 target number 41000\n",
      "1061.27 target number 42000\n",
      "1097.09 target number 43000\n",
      "1134.18 target number 44000\n",
      "1170.18 target number 45000\n",
      "1206.81 target number 46000\n",
      "1244.89 target number 47000\n",
      "1283.28 target number 48000\n",
      "1322.15 target number 49000\n",
      "1362.80 target number 50000\n",
      "1402.58 target number 51000\n",
      "1442.98 target number 52000\n",
      "1484.91 target number 53000\n",
      "1526.52 target number 54000\n",
      "1568.16 target number 55000\n",
      "1611.11 target number 56000\n",
      "1654.79 target number 57000\n",
      "1700.60 target number 58000\n",
      "1746.69 target number 59000\n",
      "1792.97 target number 60000\n",
      "1839.11 target number 61000\n",
      "1885.81 target number 62000\n",
      "1932.06 target number 63000\n",
      "1981.70 target number 64000\n",
      "2030.87 target number 65000\n",
      "2080.56 target number 66000\n",
      "2130.79 target number 67000\n",
      "2182.15 target number 68000\n",
      "2233.73 target number 69000\n",
      "2287.81 target number 70000\n",
      "2342.40 target number 71000\n",
      "2398.50 target number 72000\n",
      "2454.69 target number 73000\n",
      "2510.77 target number 74000\n",
      "2568.85 target number 75000\n",
      "2626.54 target number 76000\n",
      "2685.29 target number 77000\n",
      "2742.36 target number 78000\n",
      "2801.73 target number 79000\n",
      "2861.43 target number 80000\n",
      "2920.67 target number 81000\n",
      "2983.94 target number 82000\n",
      "3044.73 target number 83000\n",
      "3106.26 target number 84000\n",
      "3169.08 target number 85000\n",
      "3234.77 target number 86000\n",
      "3299.76 target number 87000\n",
      "3365.94 target number 88000\n",
      "3430.65 target number 89000\n",
      "3496.87 target number 90000\n",
      "3567.93 target number 91000\n",
      "3635.11 target number 92000\n",
      "3704.35 target number 93000\n",
      "3774.83 target number 94000\n",
      "3843.17 target number 95000\n",
      "3913.11 target number 96000\n",
      "3983.68 target number 97000\n",
      "4057.80 target number 98000\n",
      "4131.90 target number 99000\n",
      "4204.25 target number 100000\n",
      "4279.98 target number 101000\n",
      "4352.91 target number 102000\n",
      "4426.55 target number 103000\n",
      "4501.99 target number 104000\n",
      "4577.68 target number 105000\n",
      "4654.63 target number 106000\n",
      "4734.33 target number 107000\n",
      "4812.57 target number 108000\n",
      "4904.53 target number 109000\n",
      "4986.91 target number 110000\n",
      "5068.78 target number 111000\n",
      "5148.46 target number 112000\n",
      "5228.55 target number 113000\n",
      "5313.65 target number 114000\n",
      "5394.74 target number 115000\n",
      "5481.39 target number 116000\n",
      "5565.46 target number 117000\n",
      "5653.53 target number 118000\n",
      "5739.37 target number 119000\n",
      "5827.50 target number 120000\n",
      "5917.75 target number 121000\n",
      "6002.77 target number 122000\n",
      "6091.32 target number 123000\n",
      "6180.91 target number 124000\n",
      "6269.77 target number 125000\n",
      "6360.00 target number 126000\n",
      "6449.08 target number 127000\n",
      "6539.65 target number 128000\n",
      "6629.33 target number 129000\n",
      "6725.07 target number 130000\n",
      "6819.55 target number 131000\n",
      "6918.34 target number 132000\n",
      "7015.68 target number 133000\n",
      "7113.73 target number 134000\n",
      "7211.64 target number 135000\n",
      "7309.50 target number 136000\n",
      "7459.56 target number 137000\n",
      "7664.85 target number 138000\n",
      "7906.27 target number 139000\n",
      "8076.78 target number 140000\n",
      "8194.65 target number 141000\n",
      "8293.95 target number 142000\n",
      "8398.57 target number 143000\n",
      "8502.21 target number 144000\n",
      "8660.91 target number 145000\n",
      "8871.68 target number 146000\n",
      "9081.73 target number 147000\n",
      "9298.68 target number 148000\n",
      "9564.07 target number 149000\n",
      "9850.40 target number 150000\n",
      "10139.29 target number 151000\n",
      "10272.81 target number 152000\n",
      "10418.74 target number 153000\n",
      "10568.29 target number 154000\n",
      "10716.05 target number 155000\n",
      "10873.67 target number 156000\n",
      "11030.20 target number 157000\n",
      "11172.13 target number 158000\n",
      "11299.96 target number 159000\n",
      "11435.57 target number 160000\n",
      "11562.74 target number 161000\n",
      "11689.33 target number 162000\n",
      "11818.93 target number 163000\n",
      "11958.33 target number 164000\n",
      "12105.20 target number 165000\n",
      "12238.86 target number 166000\n",
      "12374.78 target number 167000\n",
      "12495.67 target number 168000\n",
      "12640.50 target number 169000\n",
      "12771.08 target number 170000\n",
      "12915.94 target number 171000\n",
      "13049.07 target number 172000\n",
      "13179.87 target number 173000\n",
      "13317.04 target number 174000\n",
      "13453.58 target number 175000\n",
      "13599.12 target number 176000\n",
      "13749.45 target number 177000\n",
      "13894.97 target number 178000\n",
      "14038.66 target number 179000\n",
      "14183.17 target number 180000\n",
      "14333.86 target number 181000\n",
      "14484.68 target number 182000\n",
      "14631.99 target number 183000\n",
      "14778.44 target number 184000\n",
      "14931.26 target number 185000\n",
      "15089.12 target number 186000\n",
      "15288.67 target number 187000\n",
      "15522.29 target number 188000\n",
      "15832.95 target number 189000\n",
      "16120.52 target number 190000\n",
      "16316.21 target number 191000\n",
      "16606.26 target number 192000\n",
      "16895.05 target number 193000\n",
      "17094.44 target number 194000\n",
      "17299.78 target number 195000\n",
      "17495.31 target number 196000\n",
      "17683.42 target number 197000\n",
      "17901.81 target number 198000\n",
      "18097.04 target number 199000\n",
      "18284.00 target number 200000\n",
      "18465.63 target number 201000\n",
      "18653.51 target number 202000\n",
      "18810.18 target number 203000\n",
      "19004.73 target number 204000\n",
      "19189.61 target number 205000\n",
      "19418.73 target number 206000\n",
      "19651.50 target number 207000\n",
      "19883.04 target number 208000\n",
      "20093.96 target number 209000\n",
      "20276.88 target number 210000\n"
     ]
    }
   ],
   "source": [
    "trained_model_dir = train(n_epochs=3, \n",
    "                          embedding_dim=100, \n",
    "                          l1_corpus=training_en, \n",
    "                          l2_corpus=training_fr, \n",
    "                          vocab_size=10000, \n",
    "                          lr=1e-3,\n",
    "                          label=\"embalign-training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIZMb7p_mwFQ"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jot1EcOdzZsO"
   },
   "outputs": [],
   "source": [
    "def get_candidates(fileName='lst/lst.gold.candidates'):\n",
    "    candidates = {}\n",
    "    \n",
    "    with open(fileName, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            target, options = line.split('::')\n",
    "            candidates[target] = [x.strip() for x in options.split(';')]\n",
    "            \n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def get_test_sentences(fileName='lst/lst_test.preprocessed'):\n",
    "    \n",
    "    with open (fileName,'r') as testFile:\n",
    "        lines = testFile.readlines()\n",
    "    \n",
    "    return [line.split() for line in lines]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-91uCk25yyPx"
   },
   "outputs": [],
   "source": [
    "def run_lst(model, w2i, eval_func):\n",
    "    \n",
    "    # Load evaluation data\n",
    "    sentences = get_test_sentences('lst/lst_test.preprocessed')\n",
    "    all_candidates = get_candidates('lst/lst.gold.candidates')\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for sentence in sentences:        \n",
    "        # Encode the target withouth the POS-tag\n",
    "        target = sentence[0]\n",
    "        target_stripped = target[:-2]\n",
    "        \n",
    "        # Extract id's\n",
    "        sent_id = sentence[1]\n",
    "        targ_idx = int(sentence[2])\n",
    "        \n",
    "        # Remove target word from sentence to obtain context\n",
    "        context = sentence[3:].copy()\n",
    "        \n",
    "        scores = eval_func(model, targ_idx, all_candidates[target], context, w2i)\n",
    "        result.append(format_ranking_string(target, sent_id, scores))\n",
    "    \n",
    "    # save output to file\n",
    "    with open(os.path.join('lst', 'lst.out'), 'w') as f:\n",
    "        f.writelines(result)\n",
    "        \n",
    "    # call evaluation scripts\n",
    "    subprocess.Popen('python lst/lst_gap.py lst/lst_test.gold lst/lst.out out no-mwe'.split())\n",
    "        \n",
    "def format_ranking_string(target, sentence_id, scores):\n",
    "        \n",
    "    base = \"RANKED\\t{} {}\".format(target, sentence_id)\n",
    "    candidates = \"\".join(\"\\t{} {}\".format(x[0], x[1]) for x in scores)\n",
    "    \n",
    "    return base + candidates + \"\\n\"\n",
    "        \n",
    "def load_model(model_dir, model_file):\n",
    "    '''model_dir: path of DIRECTORY were model is stored,\n",
    "       model_file: filename of epoch you want to load'''\n",
    "    \n",
    "    with open(os.path.join(model_dir, 'w2i.pickle'), 'rb') as f:\n",
    "        w2i = pickle.load(f)\n",
    "        w2i = defaultdict(lambda: w2i['<unk>'], w2i)\n",
    "    \n",
    "    with open(os.path.join(model_dir, model_file), 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "        \n",
    "    return model, w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_embalign(model, targ_idx, candidates, context, w2i):\n",
    "    # Function to evaluate the Embed Align model\n",
    "    # INPUT\n",
    "    # - model, the model which is alreadly loaded from a .pickle file\n",
    "    # - targ_idx, an integer corresponding to the idx of the target word\n",
    "    # - candidates, a list of suitable candidates\n",
    "    # - context, the entire sentence\n",
    "    # - w2i, word2index dict\n",
    "    #\n",
    "    # OUTPUT\n",
    "    # - scores, a list containing tuples (candidate_word,score)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    # Convert sentence to idx and LongTensor\n",
    "    idx_sent = [w2i[word] for word in context]\n",
    "    idx_sent = torch.LongTensor(idx_sent)\n",
    "    \n",
    "    # Run a forward pass\n",
    "    _, _, mu_targ, sigma_targ = model.forward(idx_sent)\n",
    "        \n",
    "    # Retrieve mu[target_idx] and sigma[target_idx] \n",
    "    mu_target_idx = mu_targ[targ_idx,:]\n",
    "    sigma_target_idx = sigma_targ[targ_idx,:]\n",
    "    \n",
    "    # Retrieve the target distribution\n",
    "    target_dist = MultivariateNormal(mu_target_idx,torch.diag(sigma_target_idx))\n",
    "    \n",
    "    for c_word in candidates:\n",
    "        \n",
    "        # Create the new sentece, and replace a word\n",
    "        new_sent = copy.copy(context)\n",
    "        new_sent[targ_idx] = c_word\n",
    "        \n",
    "        # Convert to idx and LongTensor\n",
    "        idx_new_sent = [w2i[word] for word in new_sent]\n",
    "        idx_new_sent = torch.LongTensor(idx_new_sent)\n",
    "        \n",
    "        # Run a forward pass\n",
    "        _, _, mu_cand, sigma_cand = model.forward(idx_new_sent)\n",
    "        \n",
    "        # Retrieve mu[target_idx] and sigma[target_idx] \n",
    "        mu_cand_idx = mu_cand[targ_idx,:]\n",
    "        sigma_cand_idx = sigma_cand[targ_idx,:]\n",
    "        \n",
    "        cand_dist = MultivariateNormal(mu_cand_idx,torch.diag(sigma_cand_idx))\n",
    "    \n",
    "        # Calculate kl_divergence and convert to float\n",
    "        kl_div = float(kl_divergence(target_dist,cand_dist))\n",
    "        \n",
    "        scores.append((c_word,kl_div))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1Q-XGiaJ0vKW"
   },
   "outputs": [],
   "source": [
    "# RUN LST ON TRAINED MODEL\n",
    "model_dir = os.path.join('dev_en_lr-0.01_emb-100_window-6_batch-50_vocab-251')\n",
    "trained_model, w2i = load_model(model_dir, 'ep-9.pickle')\n",
    "run_lst(trained_model, w2i, evaluate_embalign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hnGHTvcMOYNW"
   },
   "outputs": [],
   "source": [
    "bank = torch.LongTensor([w2i['street']])\n",
    "embs = trained_model.mu_prior.weight\n",
    "sims = []\n",
    "for i in itertools.chain(*dev_en):\n",
    "    word = torch.LongTensor([w2i[i]])\n",
    "    sim = F.cosine_similarity(embs[bank], embs[word])\n",
    "    sims.append((i, sim.item()))\n",
    "\n",
    "unk = \"<unk>\"\n",
    "word = torch.LongTensor([w2i[unk]])\n",
    "sim = F.cosine_similarity(embs[bank], embs[word])\n",
    "sims.append((unk, sim.item()))\n",
    "print(sorted(sims, key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VcE2U_99EFvI"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_emba_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-338724a0b4d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_emba_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_en\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_fr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i_l1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i_l2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0msave_alignments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'python wa/aer.py wa/test.naacl wa/aer_out.txt'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_emba_data' is not defined"
     ]
    }
   ],
   "source": [
    "def predict_alignment(model, idx, sentence1, sentence2):\n",
    "    '''Sentence1: LongTensor corresponding to sentence in l1 language\n",
    "       sentence2: LongTensor corresponding to sentence in l2 language\n",
    "       model: EmbedAlign instance to evaluate\n",
    "       \n",
    "       return: list of alignments [(sent_idx, w1_idx, w2_idx, prob)]'''\n",
    "    \n",
    "    alignment = []\n",
    "    \n",
    "    # Do forward pass and take (arg)max\n",
    "    _, l2_probs, _, _ = model(sentence1)\n",
    "    \n",
    "    # make a list of the results\n",
    "    for l2_idx, lookup in enumerate(sentence2):\n",
    "        l2_word_probs = l2_probs[:, lookup.item()]\n",
    "        log_prob, l1_idx = l2_word_probs.max(0)\n",
    "        prob = torch.exp(log_prob)\n",
    "        \n",
    "        # correct to 1-indexation\n",
    "        l1_idx = l1_idx.item() + 1\n",
    "        l2_idx += 1\n",
    "        alignment_data = [idx, l1_idx, l2_idx, prob.item()]\n",
    "        alignment_data = [str(x) for x in alignment_data]\n",
    "        alignment_data += ['\\n']\n",
    "        \n",
    "        alignment.append(' '.join(alignment_data))\n",
    "        \n",
    "    return alignment\n",
    "\n",
    "def save_alignments(model, data):\n",
    "    \n",
    "    alignments = []\n",
    "    i = 1\n",
    "    for s1, s2 in data:\n",
    "        \n",
    "        # Get all alignments for these sentences\n",
    "        sentence_alignment = predict_alignment(model, i, s1, s2)\n",
    "        alignments += sentence_alignment\n",
    "        \n",
    "        i += 1\n",
    "   \n",
    "    # write list to file\n",
    "    with open('aer_out.txt', 'w') as f:\n",
    "        f.writelines(alignments)\n",
    "        \n",
    "\n",
    "test_data = generate_emba_data(test_en, test_fr, w2i_l1, w2i_l2)\n",
    "save_alignments(trained_model, test_data)\n",
    "subprocess.Popen('python wa/aer.py wa/test.naacl wa/aer_out.txt'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ttawX-00nzeX"
   },
   "outputs": [],
   "source": [
    "w2i_en['poep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8gd_qq1hSHzX"
   },
   "outputs": [],
   "source": [
    "# test_model = BayesianSkipGram(10, 3)\n",
    "torch.max(trained_model.embed.weight)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Embed-align",
   "provenance": [
    {
     "file_id": "1cIymPrqXYwCI_AFpHQm2iYeYF9VsHElv",
     "timestamp": 1525872474801
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
